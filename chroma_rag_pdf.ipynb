{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3699f76f-4107-47c6-9bb6-b11785278605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3220d2d7-d513-4bed-9a86-e4141293700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.environ.get('GROQ_API_KEY')\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b14d35c2-18e9-4e5f-a31e-447fb26833c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef = chromadb.utils.embedding_functions.DefaultEmbeddingFunction()\n",
    "\n",
    "class DefChromaEF(Embeddings):\n",
    "  def __init__(self,ef):\n",
    "    self.ef = ef\n",
    "\n",
    "  def embed_documents(self,texts):\n",
    "    return self.ef(texts)\n",
    "\n",
    "  def embed_query(self, query):\n",
    "    return self.ef([query])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fecd3d2-6fe8-4866-b8cb-ec1b31625b8d",
   "metadata": {},
   "source": [
    "# Get vector store | retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc2ac34f-30aa-47c2-bfe5-d79d36c47fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"chromadb/\")\n",
    "db = Chroma(client=client, collection_name=\"2402.05123.pdf\", embedding_function=DefChromaEF(ef))\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b264e5-9a32-4575-a138-2fbaaae8357e",
   "metadata": {},
   "source": [
    "# Create prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b37aaf20-58f1-4a02-8fd2-a1906e5d7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<bos><start_of_turn>user\\nAnswer the question based only on the following context and extract out a meaningful answer. \\\n",
    "Please write in full sentences with correct spelling and punctuation. if it makes sense use lists. \\\n",
    "If the context doesn't contain the answer, just respond that you are unable to find an answer. \\\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\\n\n",
    "ANSWER:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33533a69-5ffc-465a-8988-b140a71309ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef205a-4163-44ff-8e4a-5e75b1db779f",
   "metadata": {},
   "source": [
    "# Create the RAG chain using LCEL with prompt printing and streaming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d64c9a21-8acf-45e8-acf4-1638be7be03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0ccdf-c427-402c-8132-5692967701b8",
   "metadata": {},
   "source": [
    "# Function to ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51a2be4f-8512-4cb5-bcf4-fd3edd7368f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    print(\"Answer:\\n\\n\", end=\" \", flush=True)\n",
    "    for chunk in rag_chain.stream(question):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d263543-f0f6-46a2-82f0-d93f037cf954",
   "metadata": {},
   "source": [
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b87c7a-25df-451b-b682-0534714a70a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  what is fine-tuning?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " Based on the provided context, fine-tuning refers to the process of adjusting a large language model (LLM) to a specific task or dataset. This is done by training the LLM on a small, high-quality subset of the dataset, which is carefully selected to improve the model's performance on that task.\n",
      "\n",
      "In the context of instruction tuning, fine-tuning involves training the LLM on a subset of the instruction dataset to enhance its ability to follow human instructions. This process is crucial in aligning the LLM with human intent and improving its controllability and safety.\n",
      "\n",
      "The fine-tuning process typically involves several steps, including:\n",
      "\n",
      "1. Selecting a high-quality subset of the instruction dataset using a data selection method.\n",
      "2. Training the LLM on the selected subset.\n",
      "3. Evaluating the LLM's performance on a testing set.\n",
      "\n",
      "Fine-tuning is a crucial step in the instruction tuning process, as it enables the LLM to adapt to specific domains and tasks, reducing the need for large amounts of data and computational resources.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  summarize the text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " The text discusses various methods for selecting and filtering instruction data for large language models (LLMs) to improve their performance and efficiency. The methods include using powerful LLMs like ChatGPT, generating and filtering instructions, and selecting high-quality instruction data. The text also mentions several papers and conferences related to this topic.\n",
      "\n",
      "The methods for selecting instruction data include:\n",
      "\n",
      "1. Using powerful LLMs like ChatGPT to assess the quality of each data tuple.\n",
      "2. Generating and filtering instructions using techniques like instruction backtranslation and self-alignment.\n",
      "3. Selecting high-quality instruction data using methods like dataset quantization and core-set approach.\n",
      "\n",
      "The text also mentions several papers and conferences related to this topic, including papers on instruction mining, dataset quantization, and model-oriented data selection.\n",
      "\n",
      "Overall, the text provides an overview of the current state of research in selecting and filtering instruction data for LLMs, highlighting the importance of high-quality instruction data for improving the performance and efficiency of LLMs.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  whar are the main findings mention in the text?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " Based on the provided context, I was unable to find an answer to the question \"what are the main findings mentioned in the text?\" because there is no specific mention of findings or results in the text. The text appears to be an introduction to a research paper and presents an overview of the topic, methodology, and approaches used in the research.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  what are the main findings mention in the text?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " Based on the provided context, the main findings mentioned in the text are:\n",
      "\n",
      "1. The performances of different selection methods on winning rates are summarized in Table 1.\n",
      "2. The inner comparisons of LLMs tuned on subsets with themselves tuned on full sets are summarized in Table 2.\n",
      "3. The external comparisons of LLMs with external LLMs on different testing benchmarks are summarized in Table 3.\n",
      "4. The INSTRUCTMINING method uses a linear equation to calculate the quality of instructions and can be used for data selection.\n",
      "5. The InstructionGPT-4 method uses a trainable data selector to process and assign scores to each piece of instruction fine-tuning data.\n",
      "6. The DQ method uses a gain function to partition the dataset into non-overlapping subsets and select a representative sample from each subset.\n",
      "7. The IFD method uses an LLM to initially learn to recognize instructions from carefully selected data and enhance its ability to assess broader dataset quality and estimate instruction-following difficulty.\n",
      "\n",
      "These findings are primarily related to the development and evaluation of methods for data selection in instruction tuning for large language models (LLMs).\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  give overview on the model-oriented data selection mention in the text.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " Based on the provided context, the model-oriented data selection methods mentioned in the text are:\n",
      "\n",
      "1. **System of Indicators**: This method employs a metric set system directly to identify multiple metrics, such as instruction length, complexity, and reward scores. These metrics are used to calculate individual scores for each data instance, and then the scores are used to develop a more robust metric set system.\n",
      "2. **Trainable LLMs**: This method uses a trainable LLM, such as LLaMa, to develop computation formulas in data selection processes. The LLM processes and assigns scores to each piece of instruction fine-tuning data.\n",
      "3. **Powerful LLMs**: This method uses powerful external LLMs, such as ChatGPT, to select instructions.\n",
      "4. **Small Models**: This method uses smaller models, such as MoDS, and designs a comprehensive process to achieve a balanced effect across all aspects.\n",
      "\n",
      "These model-oriented data selection methods are used to select high-quality instruction data for fine-tuning large language models (LLMs). The methods aim to reduce training costs and enhance the instruction-following capabilities of LLMs.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  what are the measures for indicator use in the text?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " Based on the provided context, the measures for indicator use are:\n",
      "\n",
      "1. **Wining Rate**: Calculated as `(Num(win)−Num(lose))/Num(all)+1` (Formula 10), it evaluates the effectiveness of the dataset selection methods.\n",
      "2. **Prompt Uncertainty**: Determined by generating k perturbed instructions through the random deletion of words from an original instruction, it serves as a measure of task uncertainty.\n",
      "3. **KLIP Score**: Used in [Radford et al., 2021] to assess the quality of instruction data.\n",
      "4. **Complexity**: Described by factors such as length, difficulty, and complexity of the instruction.\n",
      "5. **Quality**: Captures the accuracy of the output.\n",
      "6. **Quality labels**: Assigned post fine-tuning and evaluation of each subset.\n",
      "7. **IFD (Instruction Following Difﬁculty)**: A metric introduced by [Liet al., 2023a] to assess the challenge of responding to specific instructions.\n",
      "\n",
      "These measures are used to evaluate the quality, diversity, and complexity of the instruction data and to select the most suitable subset for fine-tuning the large language models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question (or type 'quit' to exit): \")\n",
    "        if user_question.lower() == 'quit':\n",
    "            break\n",
    "        answer = ask_question(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544e125-f988-4dbb-a2f4-2ea415c6d2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
