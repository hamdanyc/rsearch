Informatics in Medicine Unlocked 30  2022  100961 Available online 4 May 2022 2352 9148   2022 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license   http   creativecommons.org licenses by 4.0   .Machine learning and deep learning based Natural Language Processing for  auto vetting the appropriateness of Lumbar Spine Magnetic Resonance  Imaging Referrals  Ali H. Alanazia b    Andrea Cradocka  John Ryanc  Louise Rainforda  aRadiography and Diagnostic Imaging  School of Medicine  University College Dublin  Ireland  bSociety of Artificial Intelligence in Healthcare  Riyadh  Saudi Arabia  cZiltron LTD  Ireland    ARTICLE INFO   Keywords   Natural language processing  Machine learning  Deep learning  Referrals   appropriateness  Magnetic resonance imaging  Lumbar spine ABSTRACT   Manual vetting of radiology referrals is an essential daily task to ensure the appropriateness of the received  referrals. Such tasks require sufficient clinical experience and may challenge the radiology staff. With the  emerging of artificial intelligence  AI  technology and advancement in natural language processing NLP  most of  the available machine learning based NLP models targeted research cohort building and healthcare quality.  Other healthcare management tasks such as auto vetting radiology referrals have not been adequately encoded.  Furthermore  challenges  including class imbalance and lack of direct comparison with humans  are yet to be  investigated sufficiently. In this study  a set of machine learning and deep learning models were developed for  auto vetting of lumbar spine magnetic resonance imaging LSMRI referrals as indicated or not indicated for  scanning using referrals from two hospitals. The impact of applying one of the text augmentation techniques on  the models   performance has been investigated. In addition  the performance of four different feature extraction  techniques has been critically analyzed. Moreover  a comparison has been conducted between the developed  models with two expert radiologists who were not involved in establishing the gold standard labels using an  unseen dataset. The results show that the models   performances significantly improved with the augmented data   with an increase in F1 scores ranging from 1  to 8 . Support vector machine with bag of words achieved the  highest AUC reaching 0.99. Convolutional neural network model achieved the second highest model with AUC   0.97. All models outperformed the two expert radiologists when comparisons were conducted on the unseen  dataset.    1.Introduction  Low back pain LBP is one of the most common complaints that lead  patients to visit doctors and is a leading cause of movement disability   1 . This increase has led many patients to be referred for Lumbar Spine  Magnetic Resonance Imaging LSMRI. However  the appropriateness of  MR scanning for the lumbar spine is gaining wide attention in the  medical field. Several studies investigated the appropriateness of Lumbar Spine MRI  LSMRI  referrals through analyzing their adherence  to the international guidelines  such as the American College of Radi  ology Appropriateness Criteria  ACR   2  and Royal College of Radiol   ogists guidelines  RCR   3   or the national and local guidelines. The  range of inappropriate LSMRI examinations is reported to be between  10  and 60   4 7 . This noticeable increase in unjustified cases has led  health organizations and governments worldwide to search for effective  approaches to reduce this phenomenon  8 9 .  Abbreviations  ACR  American College of Radiology  AI  Artificial Intelligence  AUC  Area Under the Curve  Bi LSTM  Bi directional Long Short Term Memory   BoW  Bag of Words  CDS  Clinical Decision Support  CNN  Convolutional Neural Networks  CT  Computed Tomography  LBP  Low Back Pain  LR  Logistic Regression   LSMRI  Lumbar Spine Magnetic Resonance Imaging  MRI  Magnetic Resonance Imaging  MSK  usculoskeletalM  NLP  Natural Language Processing  NLTK  Natural  Language Toolkit  RBF  Radial Basis Function  RCR  Royal College of Radiologists  RF  Random Forest  RNN  Recurrence Neural Network  SVM  Support Vector  Machine  TF IDF  Term Frequency Inverse Document Frequency.   Corresponding author. Dublin  Sandyford  South central building  apartment 29  Ireland.  E mail addresses  Ali.alanazi ucdconnect.ie  A.H. Alanazi   Andrea.cradock ucd.ie  A. Cradock   John.Ryan ziltron.com  J. Ryan   Louise.Rainford ucd.ie   L. Rainford .   Contents lists available at ScienceDirect  Informatics in Medicine Unlocked  u   zkw  s yo kr o     1ow  o to 1m y2w m k o2ty   https   doi.org 10.1016 j.imu.2022.100961  Received 24 March 2022  Received in revised form 24 April 2022  Accepted 29 April 2022   

Informatics in Medicine Unlocked 30  2022  100961 2Currently  there are two main approaches to reduce inappropriate  referrals. The first approach is to provide educational and training  courses related to the appropriateness of the referrals for radiology staff  responsible for auditing referrals  10 . However  affording such training  and educational courses may impose a financial impact on radiology  departments. Another approach is to utilize Clinical Decision Support  software CDS that provide real time feedback to the referring physicians  about the required information and referrals appropriateness. This  approach has been proven to minimize inappropriate examinations in  some studies  11 13 . However  referrers need some pre utilization  training and teaching courses on how such a tool is used as it employs  a series of questions and checklists before manual order entry  which  might also cause some reluctance to the physicians. To overcome these  obstacles and facilitate referral vetting processes to minimize unjustified  radiology requests  Natural Language Processing NLP uses machine  learning  and deep learning technology may offer an efficient and  cost effective solution by automating referral auditing.  In previous studies  the application of NLP and machine learning for  decision making has gained wide attention  especially in the radiology  field. Recent progress in machine learning based NLP has shown great  potential towards automating the classification process of free radiology  text. For example  in a study conducted by Trivedi et al.  14   the authors  successfully built a model for auto assigning the use of contrast media  for musculoskeletal MSK MRI protocols based on the clinical indications  written in the referrals with an accuracy reached 83 . In another study   15   the authors experimented with different algorithms for  auto vetting the appropriateness of brain MRI referrals and reported an  F1 score of 0.94 for the XGBoost algorithm.  To the best of the researchers  knowledge  no existing work applies  and compares machine learning with various feature extraction tech  niques and deep learning models in classifying the appropriateness of  LSMRI referrals. Also  no previous studies directly compared the models   performance against humans on unseen data. The main contributions of  this study can be presented as follow   1.1. Methodological contributions    a  This study investigated the use of three traditional machine  learning algorithms  support vector machine SVM  logistic  regression LR and random forest RF  as well as two deep learning  algorithms  convolutional neural networks CNNs and recurrence  neural network RNN with bi directional long short term memory  Bi LSTM  for auto assigning the appropriateness of LSMRI re  ferrals. To obtain word embeddings  deep learning models  CNN  and Bi LSTM  were integrated with pre trained word embedding  model  called Fasttext  which developed by Facebook research  team  available online  https   fasttext.cc docs en english vec  tors.html . Fasttext was trained on common crawl datasets  and  it contains 2 million word vectors.    b  A pre developed text augmentation technique was applied to the  minor class  the not indicated referrals  to adjust data imbalance  and investigate the influence of the augmented data on models   classification.  1.2. Comparative analysis    a  A comparative analysis has been conducted for the performance  of four features extraction techniques implemented on the orig  inal and the augmented data with the traditional models  bag of  words  bigram  trigram  and term frequency inverse document  frequency TF IDF.   b A direct comparison has been performed between the classifica   tions of the best developed models against that of two expert  radiologists who were not involved in the creation of the gold  standard labels in unseen data  not used during training and  testing . Such comparison may provide a clear view of the effectiveness of the operational application of artificial intelli   gence models in radiology departments.  2.Methods and materials  2.1. Dataset  This study was approved by the institutional review board of Uni  versity College Dublin UCD. All related agreements and permissions for  data collection were granted. A total of 1020 LSMRI referrals  including  patient  demographics and written indications  were extracted in the  form of  referral texts  from two hospitals in Ireland  one public and  another private  593 and 427  respectively. The reason why data were  collected from two hospitals is to include different language styles in the  corpus. The public hospital accepted referrals internally from medical  registrars or higher  whilst the private hospital included acceptance  from general practitioners  external clinical surgeries  and internal  consultants. All irrelevant material  colloquialisms and material per  taining to the local service and patients  identities were edited to ensure  referrals were clear and de identified.  2.1.1. Creation of the gold standard labels for referrals in the dataset  To obtain the gold standard labels for the referrals in the dataset   section 2.1   three MSK radiologists with experiences ranging from 4.5  to 8 years after board certification were recruited. The participants were  requested to label each referral as indicated or not indicated for MR  scanning independently based on their clinical experiences. The par  ticipants were given part of the data every week for classification. Data  source  public or private  was hidden from the participants to avoid any  conferring biases. After all classifications were obtained  the gold stan  dard was created based on the majority vote for each case. The gold  standard included 221  21.7   not indicated  and 799  78.3   indi  cated referrals for scanning.  2.1.2. Split of the dataset  The referrals with their gold standard labels in the dataset  section  2.1.1  were divided into two subsets. The first subset contained 920  referrals  199 not indicated and 721 indicated  which used for the  development of the models  called original subset. The second subset  contained 100 referrals  78 indicated and 22 not indicated  and was  used as a held out subset  unseen subset  for the final comparison with  radiologists.  2.1.3. Creation of the augmented subset  As there was severe class imbalance in the original subset in section  2.1.2  199 not indicated vs 721 indicated referrals . The decision was  made to create an augmented subset by only augmenting the not indi  cated referrals  minor class  in the original subset. A pre developed  customized system  available online  https   www.kaggle.com shone  nkov nlp albumentations   was used to swap the positions of the  words. The system was designed using the NLP albumentation package  to take the original document and generate new documents by swapping  the words positions. For multiple generations of a single document  as  implemented in this study  the process was implemented 3 consecutive  times  and in each time  a new text was generated. The system requires a  manual entry of the variables swap distance and swap probability for each  new text  which are responsible for swapping distance and probability  for swapping one word. Fig. 1 illustrates an example of a not indicated  referral that was augmented three times. Hence  the number of not  indicated referrals in the augmented subset increased from 199 to 796   and the total referrals in this subset became 1517  796 not indicated vs  721 indicated . A.H. Alanazi et al.                                                                                                                                                                                                                              

Informatics in Medicine Unlocked 30  2022  100961 32.2. Data pre processing  2.2.1. Data cleaning  All referral texts were cleaned using the Natural Language Toolkit  NLTK as follow    1 Removal of stop wards  common stop words in the referrals that do  not add any value to the classification  such as  the  a  an  on  in    were removed using the stop words removal function. Negation  words found in the referrals were retained.   2 Removal of punctuations  all unnecessary punctuations were  removed  and the meaningful ones with temporal connotations  such  as  D  F    were retained.   3 Lowercase  capital letters were converted to small letters.  2.2.2. Text vectorization  Machine Learning models  classifiers  cannot understand the written  text  and every text should be fed to the classifiers in the shape of a  vector with numerical values. There are several NLP techniques for  vectorization. In this study  each subset  the original and the augmented   was experimented with three bag of words techniques  bag of words  BoW  bigram  trigram  and one weighting technique called term  frequency inverse document frequency TF IDF. Thus  four feature  extraction techniques were applied. Fig. 2 illustrates an example of how  the textual referrals were preprocessed.  2.3. Development of the models  All traditional and deep learning algorithms were chosen based on  their high performance mentioned in the NLP literature related to  radiology text. The models were developed using an open source library   Google Colab. Each algorithm was experimented with the original  subset and the augmented subset  as illustrated in the pipeline  Fig. 3 .  2.3.1. Data partitions  Both subsets  the original and the augmented  were split using 70 30  ratio for training and testing the models. 2.3.2. Development of traditional machine learning models  Three algorithms were selected to be used in the development of the  models  support vector machines  SVM   logistic regression  LR   and  random forest  RF . Each algorithm experimented with both subsets  the  original and the augmented. Also  each algorithm experimented with the  four vectorization techniques mentioned in section 2.2.2 . Hence  the  number of the developed models is 24 traditional models. The grid  search was applied for the optimal parameter choices in each model. In  SVM three kernels were searched  linear  polynomial and the radial basis  function kernel  RBF . Other SVM parameters with various values were  searched  including C regularization and gamma. For logistic regression   two parameters were searched  C regularization and L1  L2 penalties.  Four parameters were searched in the random forest model  max depth   max feature  min sample leaf  and n estimator.  2.3.3. Development of deep learning models  Two algorithms were selected to be used in the development of the  deep learning models  CNN and Bi LSTM. Each algorithm experimented  with both subsets  the original and the augmented. Hence  the number of  the developed models is 4 deep learning models.  2.3.3.1. Architectures of deep learning models. Fasttext word embed   dings was integrated into both CNN models  the model trained on the  original subset and the model trained on the augmented subset  to  convert words into vectors with 300 dimensions. This was followed by a  single CNN layer with 50 neurons  3 3 convolutions  and Relu activation  in both models. The global max pooling was applied in both models to  obtain the maximum value of the convolutions in the CNN layer. A fully  connected layer with 50 neurons and Relu activation was placed after  the global maximum pooling to flatten its output in both models. The  output layer  in both models  contained 1 neuron with sigmoid function  applied to output binary predictions. Both models were compiled using  adaptive moment estimation optimizer  Adam  with learning rate  2e   2  and binary cross entropy loss function. Manual optimization was  performed for both models using batch size  180  epochs  30  and 0.5  dropout after each layer for the model trained on the original subset and  0.3 dropout after each layer for the model trained on the augmented  Fig. 1.Sample of not indicated referral before and after augmentation using swapping words technique.   Fig. 2.Illustration of the preprocessing applied to the referrals.  A.H. Alanazi et al.                                                                                                                                                                                                                              

Informatics in Medicine Unlocked 30  2022  100961 4subset.  For Bi LSTM models  Fasttext was integrated into both Bi LSTM  models  the model trained on the original subset and the model  trained on the augmented subset . This was followed by a single Bi   LSTM layer with 50 neurons for the model trained on the original sub  set and 8 neurons for the model trained on the augmented subset. A fully  connected layer with 50 neurons and Relu activation was used for the  model trained on the original subset  and 60 neurons with Relu activa   tion for the model trained on the augmented subset. In both models  the  output layer included 1 neuron with sigmoid function applied to output  binary predictions. The models were compiled using Adam optimizer  with learning rate  2e 2  and binary cross entropy loss function.  Manual optimization was performed for both models using batch size   180  epochs  30  and 0.5 dropout after each layer. 2.4. Performance evaluation and selection of the best 5 models  The standard classification performance metrics such as accuracy   precision  recall  and F1 scores were measured for each model to eval  uate the models. For models comparison  the ROC curves were plotted   and the Area Under the Curve AUC was used with F1 score as evaluation  metrics to select the best 5 models.  2.5. Final comparison with radiologists  Two MSK radiologists  other than those designated to establish the  gold standard  with minimum experience of 10 years after board certi  fication  were recruited to classify the referrals in the held out subset.  The gold standard labels of those referrals and their sources were hid  den  and radiologists were asked to assign their appropriateness as  indicated or not indicated for MR scanning independently based on their  Fig. 3.A diagram showing the pipeline for data pre processing  models   development  and final comparison.  Note   NI  not indicated  I  indicated for scanning. A.H. Alanazi et al.                                                                                                                                                                                                                              

Informatics in Medicine Unlocked 30  2022  100961 5clinical experience. The highest performing models  the best 5 models   were also re evaluated on the held out subset. The researchers intended  to keep the held out subset quite challenging with imbalance distribu   tion as there are only 22 not indicated referrals vs 78 indicated. Both  radiologists and models   outputs were benchmarked to the gold stan  dard labels of the 100 referrals  Fig. 3 . In addition  Cohen  s kappa  agreement analysis was performed between each classification  radiol   ogists   classification and models   classification  and the gold standard.  Levels of agreement were following the suggested levels by McHugh   16  in which kappa D0.00 is interpreted as no agreement  0.01  0.20 as slight  0.21  0.40 as fair  0.41  0.60 as moderate  0.61  0.80 as substan   tial  and 0.81  1.00 as perfect agreement.  3.Results  3.1. Traditional machine learning models  Random forest model with TF IDF  RF TF IDF  achieved the highest  performance among models trained and tested on the original subset  with AUC of 0.813 and F1 of 0.917. Random forest was optimized with  Table 1  Training and testing results of all developed models using the original and augmented subsets.  A.H. Alanazi et al.                                                                                                                                                                                                                              

Informatics in Medicine Unlocked 30  2022  100961 6200 estimators  70 maximum depth  squared maximum features  and 1  minimum samples leaf. For the models trained and tested on the  augmented subset  SVMs with bag of words techniques  SVM BoW DA   SVM Bigram DA  SVM Trigram DA  achieved the highest AUCs and F1  scores reached 0.999 and 0.961  respectively. SVMs were optimized  with the radial basis function kernel  RBF   a regularization value of C   5  and gramma  0.1. Table 1 shows the performance metrics of all  traditional models. Fig. 4 presents the area under the ROC curve and the  confusion matrix of the best traditional machine learning models.  3.2. Deep learning models  Bi LSTM outperformed CNN with AUC  0.794 and F1  0.925 when  models developed using the original subset. However  when the  augmented subset was used for the development  CNN DA outperformed  Bi LSTM DA  achieving an AUC of 0.975 and an F1 score of 0.918.  Performance metrics of all deep learning models developed are shown in  Table 1. Fig. 5 presents the area under the ROC curve and the confusion  matrix of the best deep learning models. 3.3. The best 5 models  All the best 5 models are from the models trained and tested on the  augmented subset  three traditional models  SVM BoW DA  SVM   Bigram DA  SVM Trigram DA  and two deep learning models  CNN   DA and Bi LSTM DA . Those models were selected based on their high  AUCs and F1 scores among other models.  3.4. Final comparison  models vs radiologists  All the best five models from section 3.3 demonstrated higher per  formances than the two radiologists when re evaluated on the held out  subset. Bi LSTM DA did show higher accuracy and F1 score than other  models. Kappa analysis showed higher agreement between the classifi   cations of the models and the gold standard than between the radiolo   gists and the gold standard. Table 2 outlines models and radiologists    classification results and the kappa agreement between each classifica   tion and the gold standard labels.  Fig. 4.AUC ROC and confusion matrix of the best machine learning models with data augmentation.  Note   There is    0.05 change in the score of the plotted AUC. A.H. Alanazi et al.                                                                                                                                                                                                                              

Informatics in Medicine Unlocked 30  2022  100961 74.Discussion  4.1. General discussion  In this work  different machine learning and deep learning models  were developed using several features extraction techniques and data  augmentation. The machine learning and deep learning approaches  demonstrated the ability to auto vet the appropriateness of lumbar spine  MRI referrals. These findings indicate that the applications of machine  learning and deep learning based NLP in referral texts could improve the  management of referrals in radiology departments.  Nevertheless  the main focus of much of the research experimenting  with the applications of machine learning and deep learning in radi  ology text analysis has been aimed to improve health care quality and research cohort building  17 . However  there is great potential for the  applications of machine learning and deep learning in healthcare man  agement. As the not indicated referrals continue to flow to radiology  departments  endpoints  such as patient safety and resource utilization   are becoming more important to healthcare providers and patients.  In the original subset that was used for the development  the classes  were imbalanced as there were 199  21.6   referrals labelled as not  indicated  negative minority class  and 721  78.4   referrals labelled as  indicated for MRI  positive majority class . The ratio of positive referrals  over negative referrals was 3.6  721 199  3.6 . Because referrals were  randomly sampled from the participating radiology departments  this  imbalance is expected  in other words  the data imbalance is a direct  result of the data nature  18 .  The fundamental issue with training a model on imbalanced data is  Fig. 5.AUC ROC and confusion matrix of the best deep learning models with data augmentation.  Note   There is    0.05 change in the score of the plotted AUC.  Table 2  Classification results of the top 5 models and the two radiologists on the unseen data. And the agreement between each classification with the gold standard labels.   Models Acc Pre Recall F1 Agreement with the gold standard of the 100 referrals  Kappa score  95  CI Agreement level  SVM BoW DA 0.84 0.87 0.92 0.90 0.50  p D0.001  0.29  0.71 Moderate  SVM Bigram DA 0.79 0.85 0.88 0.86 0.35  p D0.001  0.13  0.57 Fair  SVM Trigram DA 0.79 0.85 0.88 0.86 0.35  p D0.001  0.13  0.57 Fair  CNN DA 0.81 0.87 0.88 0.87 0.43  p D0.001  0.22  0.65 Moderate  Bi LSTM DA 0.85 0.88 0.92 0.90 0.54  p D0.001  0.33  0.74 Moderate  Radiologists  Radiologist 1 0.59 0.76 0.67 0.71  0.04  p  0.669   0.22 0.14 No  Radiologist 2 0.74 0.83 0.87 0.84 0.24  p  0.015  0.02  0.46 Fair  Note  Acc  accuracy  pre  precision  Rec  recall  F1  F1 score  and CI  confidence interval. A.H. Alanazi et al.                                                                                                                                                                                                                              

Informatics in Medicine Unlocked 30  2022  100961 8that this imbalance could lead to overfitting  in which the model per  formance becomes much lower on the testing data than its performance  on the training data  19 . To better understand the effect of imbalanced  datasets for the classification tasks  the models experimented with the  original subset  the imbalanced subset  and the augmented subset  in  which the ratio of positive referrals over the negative referrals was  adjusted to be 0.90. Data augmentation techniques are used to adjust the  distribution of the dataset classes to balance the number of positive and  negative instances  20 . As there are several techniques for data over  sampling  the words swapping technique was the choice to apply to the  original subset to help boost the signals of negative class  not indicated  referrals . It was noticed that the models trained on the original subset  suffered from severe overfitting  whereas utilizing the augmented subset  helped eliminate this issue in both traditional and deep learning models   Table 1 . Moreover  the models   performances significantly improved  with the augmented subset  with an increase in F1 scores ranging from  1  to 8 . These findings are in line with other NLP studies that aimed to  examine the effects of data augmentation on the models   performances   21 22 .  4.2. Traditional models  All traditional models developed using the augmented subset  demonstrated a high level of AUCs and F1 scores in the classification  task. SVMs with bag of words techniques  SVM BoW DA  SVM Bigram   DA  SVM Trigram DA  outperformed other traditional models with  AUCs reaching 0.99 and F1 scores up to 0.96  Table 1 . Whereas random  forest with TF IDF exhibited the highest performance with an AUC   0.81 and F1  0.91 for the original subset. This indicates that the greater  the balance between positive and negative classes within a dataset  as in  the augmented subset  the more robust performance SVM can yield  and  as the imbalance severity increases within a dataset  the performance of  SVM deteriorates accordingly  and random forest might become the best  algorithmic choice.  Bag of words features extraction techniques with SVM slightly out  performed TF IDF in the augmented subsets. These findings contradict  the findings of  23   in which the authors reported higher performance  for TF IDF with SVM over the bag of words techniques. However  in the  original subset  the TF IDF showed higher scores than bag of words. One  reason for this could be attributed to the fact that TF IDF is a word  frequency dependent technique in which the weight of a word increases  proportionally to its frequency in the referral  and as the word  commonly occurred in all referrals in the corpus  the weight is  scaled down and make it difficult to discriminate the referrals. As in the  augmented subset  only the not indicated referrals  minority class  were  augmented using the swap words technique  so each referral was  re generated three more times with the same words but different posi  tions. Thus  the frequency of any given word in those not indicated re  ferrals increased in the corpus and made some difficulties to TF IDF to  discriminate those words. Whereas in the original subset  no augmen   tation was applied  and the frequencies of words that occurred in the not  indicated referrals were low  which helped TF IDF discriminate those  words and show higher performance. In general  based on the findings   TF IDF is the best vectorization technique for binary classification with  unbalanced data  and as the balance between positive and negative  samples increases  especially with applying swapping words technique   the bag of words techniques might be the best.  4.3. Deep learning models  For the models developed using the augmented subset  CNN DA  exhibited slightly higher performance than Bi LSTM DA on the test  set. These findings are in accordance with  24 25 . Focusing only on the  CNN DA model  this study has more similarities with  26   who devel   oped a CNN model with Glove pre trained vector embedding to binary  classify CT reports as positive or negative for the presence of pulmonary embolism. Their model demonstrated an AUC of 0.99 and an F1 score of  0.938 in the internal validation set  which is slightly better than the  CNN DA  AUC  0.975  F1  0.918 . However  in their study  a larger  dataset comprising 2500 reports was used for training  whereas in this  study  the training set  after augmentation  included 1061 referrals  and  as it is known that deep neural models tend to require more data to show  good performance  25 . The high performance of the CNN DA models in  this study could be attributed to two things   1  balancing the ratio of  positive and negative referrals within the augmented subset and  2   adopting the Fasttext word vectorization  word embedding . Fasttext is  suitable for handling out of vocabulary words  OOV   as commonly exist  in medical texts. Fasttext breaks each word in the corpus into character  n grams and looks for similarities with already learned vectorization of  words in the vocabulary corpus. Unlike the Glove embedding  which  directly generates the vector of a whole word and if the word does not  exist in the vocabulary corpus that the Glove already trained on  it turns  as an out of vocabulary word  and hence influence the final output  prediction  27 28 .  Both CNN and Bi LSTM models trained on the original subset  in  which 23  of the referrals in the training set were not indicated  could  classify at least 50  of the not indicated referrals in the test set correctly.  However  CNN misclassified more cases than Bi LSTM  which reflects  the inferior F1 score of CNN compared to Bi LSTM  0.908 and 0.925   respectively  Table 1 . Although not satisfactory  these findings indicate  that Bi LSTM might be better than CNN in binary classification tasks  with imbalanced datasets  especially if the focus is to classify the mi  nority class correctly.  4.4. Comparison with radiologist experts  This is the first study that explicitly compares the performance of  machine learning models with human experts. In most of the compar   ative studies  indirect comparisons between models and humans were  conducted in which the agreement between the models   predictions and  the gold standard were compared to the agreement between the anno  tators and the gold standard. However  this approach does not provide a  clear comparison as the annotators were part of the creation of the gold  standard that had been used for the development of the models  and  hence  some bias may be introduced towards the annotators. In this  study  the best selected models  top 5 models from section 3.3  were  compared with two radiologists  who were not involved in the creation  of the gold standard  on slight challenging unseen data  held out set with  100 referrals  78 indicated vs 22 not indicated . This step can also be  considered a generalizability test for the models as the held out subset  contains referrals from the private hospital  which receives referrals  from other primary and surgery clinics. In general  all models out  performed the two radiologists in assigning the true labels. Also  all  models demonstrated higher agreement with the gold standard labels of  the 100 referrals than that demonstrated by the two radiologists   Table 2 . Bi LSTM DA showed the highest results with accuracy  0.85  and F1  0.90 and correctly classified 13 out of 22 not indicated re  ferrals. SVM BoW DA demonstrated the second highest performance  with accuracy  0.84  F1  0.90 and correct classification of 12 not  indicated referrals. Whereas the accuracy and F1 of the best radiology  classifications  the second radiologist  reached 0.74 and 0.84  respec   tively  with correct classification of 9 out of 22 not indicated cases.  However  the goal of this comparison is not to prove the superiority of  artificial intelligence models over radiologists but rather to prove their  effectiveness in the medical field and highlight their accuracies by  comparing them with experts   performances. These results indicate the  ability of machine learning and deep learning based NLP models in  vetting LSMRI referrals with at least equivalent performance to the  expertise in the field. A.H. Alanazi et al.                                                                                                                                                                                                                              

Informatics in Medicine Unlocked 30  2022  100961 94.5. Analysis of misclassified referrals in the held out subset by the top 5  models  Errors by the highest traditional models  SVMs  were manually  analyzed. It has been noticed that referrals for multiple examinations or  including irrelevant information to the lumbar spine were the main  source of false positive and false negative predictions. Examples of false   positive errors   Post heart TX. Patient has AICD not clear if it is compatible  with MRI. He needs a lumbar spine MRI for persistent pain   and  Hip pain  exam unremarkable  vascular intact    LSP stenosis    those referrals  included information related to heart transplant and hip pain which  presumably mislead the classifiers. A similar cause was also noticed in  the false negative errors. As examples   Patient is complaining of cervical    upper limb radiculopathy  mid back pain and lumbar   lower limb radi  culopathy    and  Thoraco lumbar back pain  mild scoliosis D17    both  referrals included information related to cervical radiculopathy and  thoracic pain  and the models could not infer the related information to  the lumbar spine  and hence  those referrals were incorrectly classified  as not indicated.  Ordinarily  errors in deep learning classifiers are difficult to interpret  because classifiers are considered black boxes. However  a method to  display the misclassified referrals in the shape of a heatmap  called  sensitivity analysis  29   was developed to understand the error sources.  This heatmap visualizes the referral texts and gives each word in the  document a shade of blue color so that the word in dark blue means that  this word was taken into account by the model in classifying the referrals  and vice versa. In general  it is apparent that in both models  CNN DA  and Bi LSTM DA  the temporal codes were ignored. Those codes have  important meanings for determining the pain duration and thus play a  key role in deciding the appropriateness of the referrals. Fig. 6 repre   sents the inputs of false positive and false negative referrals predicted by  the models. In both errors  the classifiers were unable to capture the time  codes and placed no color on them  which misled the classifiers and  caused false positive and false negative errors.  This study has several limitations. First  the used data size  even after  augmentation  was small and might be much better using a bigger data  size to improve the learning of the models. Second  some referrals in the  held out subset contained indications for multiple examinations irrele   vant information to the lumbar spine  which did cause some confusion to  the models and resulted in prediction errors. Third  the comparison  between models and radiologists was conducted on a small held out  subset  only 100 referrals. It would have been better if a larger data  size had been used. However  the researchers   intention was to reflect  the reality of the medical data by keeping some imbalance to the held   out subset and see how the models could deal with it. Fourth  this study did not investigate using more complex architectures such as  attention based RNN and multi channel CNN  which might solve the  issue of timing codes ignorance reported in the error analysis.  5.Conclusion  This study reports the results of several traditional and deep learning  models developed using two data subsets  original and augmented  with  different features extraction techniques for auto auditing the appropri   ateness of LSMRI referrals. Data augmentation positively impacts  models   performance  and models trained on the augmented subset  showed the ability to classify the held out referrals with higher accuracy  than two radiologists. These findings indicate that machine learning and  deep learning applications for auditing the scanning eligibility are  feasible and could improve the management of the referrals.  Ethical approvals  Ethical approvals were obtained from the relevant institutional re  view from University College Dublin  Reference Numbers  LS E 19   171 Alanazi Rainford  and  LS E 19 69 Alanazi Rainford .  Data availability  The datasets used and or analyzed during the current study are  available from the corresponding author on reasonable request.  Authors   contributions  Ali Alanazi collected the data from the hospitals  developed the  model  wrote the manuscript  and performed the analysis. Andrea Cra  dock was involved in the design  collection of the data  and text revision.  John Ryan supervised the development of the models and manuscript  revision. Louise Rainford was involved in formulating the original idea   recruitment of participants  revised the manuscript critically.  Funding  This research did not receive any specific grant from funding  agencies in the public  commercial  or not for profit sectors.  Declaration of competing interest  The authors declare that they have no known competing financial  interests or personal relationships that could have appeared to influence  Fig. 6.Image shows the sensitivity analysis  heatmap analysis  for three misclassified cases that Bi LSTM DA predicted  left  and four misclassified cases by CNN DA   right  from the unseen data. A.H. Alanazi et al.                                                                                                                                                                                                                              

Informatics in Medicine Unlocked 30  2022  100961 10the work reported in this paper.  Acknowledgements  We would like to thank all radiologists who participated in referrals   labelling.  References   1 Murray CJL  Lopez AD. Measuring the global burden of disease. N Engl J Med  2013 369 5  448 57.   2 Patel ND  et al. ACR appropriateness Criteria low back pain. J Am Coll Radiol 2016   13 9  1069 78.   3 Remedios D  France B  Alexander M. Making the best value of clinical radiology   iRefer Guidelines. In  Clin. Radiol.. eighth ed.vol. 72  2017. p. 705 7. 9.   4 Kovacs FM  et al. Appropriateness of lumbar spine magnetic resonance imaging in  Spain. Eur J Radiol 2013 82 6  1008 14.   5 Watura C  James S. Review of general practitioner direct access referrals for lumbar  spine MRI. Clin Radiol 2013 68 S5. 2013.   6 Avoundjian T  et al. Evaluating two measures of lumbar spine MRI overuse   administrative data versus chart review. J Am Coll Radiol 2016 13 9  1057 66.   7 Flaherty S  Zepeda ED  Mortele K  Young GJ. Magnitude and financial implications  of inappropriate diagnostic imaging for three common clinical conditions. Int J  Qual Health Care 2019 1 7.   8 Busse J  Alexander PE  Abdul   Razzak A  Alabousi M  Dufton J. Appropriateness of  spinal imaging use in Canada. 2013. p. 1 36.   9 Kennedy SA  Fung W  Malik A  Farrokhyar F  Midia M. Effect of governmental  intervention on appropriateness of lumbar MRI referrals  a canadian experience.  J Am Coll Radiol 2014 11 8  802 7.   10  Wang KY  et al. Reducing inappropriate lumbar spine MRI for low back pain   radiology support  communication and alignment network. J Am Coll Radiol 2018   15 1  116 22.   11  Blackmore CC  Mecklenburg RS  Kaplan GS. Effectiveness of clinical decision  support in controlling inappropriate imaging. J Am Coll Radiol 2011 8 1  19 25.   12  Liu C  Desai S  Krebs LD  Kirkland SW  Keto Lambert D  Rowe BH. Effectiveness of  interventions to decrease image ordering for low back pain presentations in the  emergency department  a systematic review. Acad Emerg Med 2018 25 6  614 26.   13  Min A  et al. Clinical decision support decreases volume of imaging for low back  pain in an urban emergency department. J Am Coll Radiol 2017 14 7  889 99.   14  Trivedi H  Mesterhazy J  Laguna B  Vu T  Sohn JH. Automatic determination of the  need for intravenous contrast in musculoskeletal MRI examinations using IBM  watson s natural language processing algorithm. J Digit Imag 2018 31 2  245 51.  15  Zhang AY  Lam SSW  Liu N  Pang Y  Chan LL  Tang PH. Development of a radiology  decision support system for the classification of MRI brain scans. In  Proc.   5th  IEEE ACM int. Conf. Big data comput. Appl. Technol. BDCAT 2018  2019.  p. 107 15.   16  McHugh ML. Lessons in biostatistics interrater reliability   the kappa statistic.  Biochem Med 2012 22 3  276 82.   17  Pons E  Braun LMM  Hunink M  Kors J. Natural language processing in radiology  a  systematic review. Radiology 2016 279 2  329 43.   18  Haibo He EAG. Learning from imbalanced data. Stud. Comput. Intell. 2009 807 9    81 110.   19  Khalilia M  Chakraborty S  Popescu M. Predicting disease risks from highly  imbalanced data using random forest. BMC Med Inf Decis Making 2011 11 1 .   20  Abdollahi M  Gao X  Mei Y  Ghosh S  Li J. A dictionary based oversampling  approach to clinical document classification on small and imbalanced dataset. In   Proc.   2020 IEEE WIC ACM int. Jt. Conf. Web intell. Intell. Agent technol. WI IAT  2020  2020. p. 357 64.   21  Abulaish M  Sah AK. A text data augmentation approach for improving the  performance of CNN. In  2019 11th int. Conf. Commun. Syst. Networks   COMSNETS 2019  2019. p. 625 30.   22  Wei J  Zou K. EDA  easy data augmentation techniques for boosting performance  on text classification tasks. In  EMNLP IJCNLP 2019   2019 conf. Empir. Methods  nat. Lang. Process. 9th int. Jt. Conf. Nat. Lang. Process. Proc. Conf.  2020.  p. 6382 8.   23  Brown AD  Kachura JR. natural language processing of radiology reports in  patients with hepatocellular carcinoma to predict radiology resource utilization.  J Am Coll Radiol 2019 16 6  840 4.   24  Heo TS  et al. Prediction of stroke outcome using natural language processing   based machine learning of radiology report of brain MRI. J Personalized Med 2020   10 4  1 11.   25  Dahl FA  et al. Neural classification of Norwegian radiology reports  using NLP to  detect findings in CT scans of children. BMC Med Inf Decis Making 2021 21 1  84.   26  Chen MC  et al. Deep learning to classify radiology free text reports. Radiology  2018 286 3  845 52.   27  Adipradana R  Nayoga BP  Suryadi R  Suhartono D. Hoax analyzer for Indonesian  news using rnns with fasttext and glove embeddings. Bull Electr Eng Informatics  2021 10 4  2130 6.   28  Khattak FK  Jeblee S  Pou Prom C  Abdalla M  Meaney C  Rudzicz F. A survey of  word embeddings for clinical text. J Biomed Informatics X 2019 4 October    100057.   29  Arras L  Horn F  Montavon G  M ller K R  Samek W. Explaining predictions of non   linear classifiers in NLP. 2016. p. 1 7. A.H. Alanazi et al.                                                                                                                                                                                                                              

