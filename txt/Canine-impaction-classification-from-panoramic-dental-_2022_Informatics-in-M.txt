Informatics in Medicine Unlocked 30  2022  100918 Available online 16 March 2022 2352 9148   2022 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY NC ND license   http   creativecommons.org licenses by  nc nd 4.0   .Canine impaction classification from panoramic dental radiographic images  using deep learning models  Malak Aljabria b    Sumayh S. Aljameelb  Nasro Min Allahb  Jawaher Alhuthayfib   Leena Alghamdib  Nouf Alduhailanb  Reem Alfehaidb  Reem Alqarawib  Muhanad Alharekyc   Suliman Y. Shahind  Walaa Al Turkic  aComputer Science Department  College of Computer and Information Systems  Umm Al Qura University  Makkah 21955  Saudi Arabia  bDepartment of Computer Science  College of Computer Science and Information Technology  Imam Abdulrahman Bin Faisal University  P.O. Box 1982  Dammam  31441  Saudi Arabia  cDivision of Pediatric Dentistry  Department of Preventive Dental Science  College of Dentistry  Imam Abdulrahman Bin Faisal University  Dammam 31441  Saudi Arabia  dDivision of Orthodontics  Department of Preventive Dental Science  College of Dentistry  Imam Abdulrahman Bin Faisal University  Dammam 31441  Saudi Arabia    ARTICLE INFO   Keywords   Deep learning  Machine learning  Convolutional neural network  Canine impaction ABSTRACT   Maxillary canine impaction is a condition that commonly occurs in growing individuals with malocclusion  and  identifying its type is always challenging for dentists when determining whether the patient needs a surgical  intervention. To avoid severe complications  it should be detected and treated as early as possible. Classifying the  type of the canine impaction from panoramic dental radiograph requires precise measurements and expensive  training  which is time consuming. The automation of this procedure could support dentists   decision making  and save time and effort. Artificial Intelligence  AI  techniques including Machine Leaning  ML  and Deep  Learning  DL  allow researchers to extract useful insights from data and thus improve decision making. In this  research  we apply DL technologies to classify impacted canines based on the Yamamoto classification. Four deep  learning models were developed to classify the type of canine impaction from panoramic dental radiographic  images  DenseNet 121  VGG 16  Inception V3  and ResNet 50. The results show that Inception V3 outperforms  the other classifiers  with an accuracy of 0.9259. The proposed model could help dentists by automating the  canine impaction prediction process  which not only reduce time and effort for dentists but also make the process  easier and more reliable.    1.Introduction  Maxillary canines are the most commonly impacted teeth after the  third molar and affect approximately 2  of the population. Maxillary  canine impaction commonly occurs at a young age in between 1  and  3  of patients and is more common in females than males  1 2 . Canine  impaction is twice as likely to occur in the maxilla than in the mandible   and 8  of patients affected by maxillary canine impaction have bilateral  impaction. About one third of maxillary canine impaction is located  labially  whereas two thirds is located palatally  3 . The main causes of  canine impaction are genetic  localized  or systemic  4 . The treatment  options for impacted canines include no treatment  surgical exposure   orthodontic treatment  and extraction. The most effective treatment approach depends on the early diagnosis of impacted canines and the  interception of potential impaction. In addition  surgical exposure and  orthodontics must be considered in some cases. Canines play an  important role in the appropriate appearance of teeth  facial features   and functional occlusion. Therefore  orthodontists have proposed  different techniques to recover impacted maxillary canines  5 . The age  of the patient and the degree of impacted canine at diagnosis affects the  success of early interceptive treatment  6 . Ericson and Kurol  7   established that removing the primary canine tooth before the age of 11  will adjust the eruption position of the permanent canines in 91  of  cases. This happens when the canine crown position is distal to the axial  line of the lateral incisor. However  if the canine crown is mesial to the  axial line of the lateral incisor  the success level will decrease to 64 .   Corresponding author. Computer Science Department  College of Computer and Information Systems  Umm Al Qura University  Makkah 21955  Saudi Arabia.  E mail addresses  mssjabri uqu.edu.sa   msaljabri iau.edu.sa  M. Aljabri   saljameel iau.edu.sa  S.S. Aljameel   nabdullatief iau.edu.sa  N. Min Allah    2170005195 iau.edu.sa  J. Alhuthayfi   2170006635 iau.edu.sa  L. Alghamdi   2170002706 iau.edu.sa  N. Alduhailan   2170005031 iau.edu.sa   R. Alfehaid   2170006008 iau.edu.sa  R. Alqarawi   malhareky iau.edu.sa  M. Alhareky   sshahin iau.edu.sa  S.Y. Shahin   wmalturki iau.edu.sa  W. Al Turki .  Contents lists available at ScienceDirect  Informatics in Medicine Unlocked  u   zkw  s yo kr o     1ow  o to 1m y2w m k o2ty   https   doi.org 10.1016 j.imu.2022.100918  Received 5 December 2021  Received in revised form 21 February 2022  Accepted 12 March 2022   

Informatics in Medicine Unlocked 30  2022  100918 2Therefore  early detection of impacted canines in growing individuals  can prove a pivotal step in correcting impactions.  Localization of the maxillary canine depends on both clinical and  radiographic examination. It has been proposed that some clinical  diagnosis factors might be a sign of canine impaction  including delayed  eruption of the permanent canine teeth after the age of 14 or 15  exis  tence of a palatal bulge  migration of the lateral incisor  and the absence  of normal canine bulge. However  radiographic images play an impor   tant role and enable proper localization to evaluate impacted maxillary  canines. This helps to determine the extent of tooth displacement  and to  determine whether the patient would benefit from a surgical or ortho   dontic approach. Several 2 dimensional  2D  radiographic techniques   including panoramic images  periapical images  and occlusal films  can  help evaluate the location of canines  8 .  In dentistry  the two most widely used methods for canine impaction  classification are the Ericson and Kurol  7  sector classification  and the  Yamamoto et al.  9  subtype classification. The Ericson and Kurol  7   five sector classification method is based on the location of the canine in  relation to the root of the lateral incisor  as shown in Fig. 1 below.  The Yamamoto et al.  9  seven subtypes classification method is  based on the angle between the occlusal plane and the long axis of the  tooth  as shown in Fig. 2 below. The seven classes are subtypes of canine  impaction in which the model proposed in this study is responsible for  identifying to which type of impaction the input image belongs.  Patients with impacted canines often undergo lengthy orthodontic  treatment. Panoramic radiograph is a standard screening tool utilized by  orthodontists and other dental specialists alike. Early identification of  canine impaction on panoramic radiograph is crucial for dentists  as it  can facilitate the treatment phase and shorten its duration. Moreover   predicting the occurrence of canine impaction and how well a patient  responds to preventive treatment  as well as determining the appro   priate time for treatment intervention  are complicated and time   consuming processes for dentists.  Artificial intelligence  AI  technologies including Machine Learning   ML  and Deep Learning  DL  are increasingly contributing to different  medical fields. The data obtained will be analyzed to enable machines to  reveal hidden opportunities and better serve the healthcare industry. DL  is a field of AI that can mimic a human brain by using the neural network  structure of deep layers  enabling the machine to learn and gain  knowledge from previous experience and data. Neural networks are the  standard deep learning model  and are the foundation of many appli   cations  including Convolutional Neural Network  CNN   which are used  in object recognition and the classification of images in image process   ing tasks  10 . All machine learning tasks must be able to deal with the  many variations that affect the observed data. For example  pixels in a  red car image can appear as black at night. This phenomenon requires  these variations to be extracted and the ones that are not significant for  the task discarded. However  the extraction of such abstract features  from the data can be complicated. DL is a potential solution to this problem  as it can produce more abstract representations and can de fine  an image concept in terms of smaller  less complicated ones  such as  corners and edges. Neural networks are DL models of deep layers that  take a set of input values and map them to output values by providing a  new abstract representation of the input at each layer  11 .  Such intelligence based techniques have made substantial contri   butions to medical fields and have been proven to be effective in the  diagnosis of different diseases and to enable early medical interventions.  CNNs are a supervised learning mechanism that has demonstrated sig  nificant performance in medical applications. For example  in the field  of ophthalmology medicine  CNN was used to classify primary open   angle glaucoma  POAG  disease with an accuracy of 98.13 . In terms  of dentistry  it can perform multiple tasks on dental radiographic im  ages  including detection  segmentation  and classification  12 . More   over  the deep learning Fuzzy Neural Network  FNN  algorithm was  used to estimate age with an accuracy of 89 . The CNN model was also  used for tooth segmentation and labelling reached an accuracy of  98.79 . In addition  the Deep Neural Network algorithm  DNN  reached  an accuracy of 98.35  in predicting the likelihood of patients being  diagnosed with diabetes  13 . These promising results from DL provide  considerable potential and new opportunities for medical research.  Using DL to classify canine impaction is likely to reduce costs and make  the process easier and more reliable in dentistry.  Given the importance of classifying impacted canines in the early  stages  this study argues that implementing an automated machine that  classifies canine impaction by specifying its type would make a differ   ence. Specifying the type of impacted canine requires attention to ac  curate measurements which might be overlooked by dentists  along with  consideration of other factors surrounding the canine. The process of  Fig. 1.The seven subtypes of impaction. Adapted from  9 .   Fig. 2.Sector classification. Adapted from  7 .  M. Aljabri et al.                                                                                                                                                                                                                                 

Informatics in Medicine Unlocked 30  2022  100918 3measuring the angle and the consideration of surrounding factors can be  time consuming for the dentist. Moreover  dentists may have different  opinions about assessing the type of the impacted canine  as one doctor  may classify the canine to be one type while others classify it as another  type. Such disagreement on classifications can lead to misdiagnosing  impacted canines on panoramic radiographs. Int turn  this urges the  need to auto mate this process. The automation of this process will save  dentists   time and effort. It will also support decision making whenever  dentists have different opinions and measurements. Despite the impor   tance of classifying canine impaction from panoramic radiography   there is no knowledge of any research study that has applied DL models  to classify impacted canines based on the Yamamoto classification.  Therefore  this research addressed this gap by developing DL models  that classify the type of canine impaction based on the Yamamoto  classification.  In the following subsections we discuss research studies related to  deep learning with dental radiographs and canine impaction prediction  and classification.  1.1. Deep learning with dental radiographs  DL teaches a computer how to mimic a human. It allows a computer  model to learn from unstructured or unlabeled data  14 15 . CNN is a  frequently used deep learning algorithm that can take images as input   assign weights and biases to different objects in the image  and then  differentiate between them  16 . Most existing studies have used the  CNN model.  Kuwada et al.  17  aimed to evaluate the performance of three  different CNN architectures in classifying the existence of maxillary  impacted supernumerary teeth  ISTs . They collected 550 panoramic  radiographs of patients with fully erupted incisors  275 with ISTs and  275 without ISTs . They applied to their dataset two models  AlexNet  and VGG 16  for classification purposes. These two models were sup  posed to classify the manually cropped images into two classes  with IST  or without IST. In the implementation of AlexNet and VGG 16  they  manually extracted the Region of Interest  ROI  and fed it into the  model. On the other hand  the third model  DetectNet  was built for  object detection and classification purposes. The model was supposed to  detect the incisor region  which is the ROI  then classify it either with or  without IST. Their results showed that the best performing model was  DetectNet  which had an accuracy of 96 .  Minnema et al.  18  aimed to develop a mixed scale dense CNN   MS D network  model to segment bony structures in Cone Beam  Computed Tomography  CBCT  scans that are affected by metal arte  facts. A total of 20 dental CBCT scans were used. All these scans were  affected by metal artefacts. Two scans were used for testing and 18 for  training. For preprocessing  a medical engineer used global thresholding  to remove noise and metal artefacts in all CBCT scans. Gold standard  segmentations were used to train the MS D network. To measure the  segmentation performance  the MS D network leave 2 out scheme was  applied and compared with CNN architectures  such as  U Net and Re  sidual Neural Network  ResNet   and the clinical snake evolution algo  rithm. As a result  the snake evolution algorithm gives mean DSC 0.78   0.07  MS D network 0.87  0.06  U Net 0.87  0.07  and ResNet 0.86   0.05.  Ngoc et al.  19  compared the performance of various feature  extraction methods with multiple different classifiers to construct an  adaptive CNN and Bag of Visual Words dental defect recognition model   which can diagnose complications in the third molar from dental X ray  images. Their dataset consisted of 447 third molar X ray images  labeled  by dentists as either  R8 Lower     R8 Null    and  R8 Upper Lower     according to which part was affected by the third molar. For pre  processing the data  all images were converted to grayscale before  performing segmentation  edge detection  and mask ROI. Oriented FAST  and Rotated BRIEF  ORB   Scale Invariant Feature Transform  SIFT    Speed Up Robust Features  SURF   and the CNN architecture VGG16 were used as feature extraction methods. For classification  Logistic  Regression  LR   Support Vector Machine  SVM   Artificial Neural Net  works  ANN   Decision Tree  DT   Gradient Boosting  Random Forest   RF   and CNN were used. The researchers applied 10 fold  cross validation and accuracy as a performance metric. Their CNN  proposed model achieved an accuracy of 84   4   which is similar to  the performance of an experienced dentist.  Lee et al.  20  developed a model to evaluate the deep CNN algo  rithm  s ability to detect dental caries based on 3000 periapical radio   graphs. The data was split into 80  for training and validation  and 20   for testing. Before the preprocessing  the periapical radiography un  derwent calibration to standardize the contrast between grey and white  matter. The images were then resized to 299  299 pixels  and  pre trained GoogLeNet Inception v3 CNN  the third edition of Google  s  Inception CNN  was for preprocessing and transfer learning. The eval  uation criteria used are accuracy  sensitivity  specificity  positive pre  dictive value  and negative predictive value. As a result  the diagnostics  for molars achieved an accuracy of 88   whilst the corresponding figure  for premolars was 89 . For the diagnostics of both the premolars and  molars  the accuracy was 82 .  Lee et al.  21  conducted a study to evaluate the performance of deep  CNNs in classifying features of osteoporosis from dental panoramic ra  diographs. Their dataset contained 680 images  80  of which were used  for training and validating and 20  to test the model. They applied  5 fold cross validation to the training data to avoid any bias and over  fitting. In addition  a method called Grad CAM was used for feature  selection purposes. They applied image preprocessing by  down sampling all images to one size of 1200  630 pixels  before  limiting the images to the region of interest  namely the mandibular area  under the teeth. The image size at the end was 700  140 pixels.  Furthermore  they evaluated four different models of CNNs  a CNN with  three layers  VGG16  a transfer learning VGG16 with pre trained  weights  VGG16 TR   and a VGG16 model with transfer learning   fine tuning  and pre trained weights  VGG16 TR FT . The utilized  pre trained weights were applied using the ImageNet dataset  which  contains thousands of images for training and evaluating classification  models. The results of this study showed that the VGG16 TR FT model  achieved the highest performance with an Area Under Curve  AUC  of  85.8   a sensitivity of 90   a specificity of 81.5   and accuracy of 84 .  However  the CNN with the three layer model showed the lowest per  formance among all used models.  1.2. Canine impaction  Alqerban et al.  22  conducted a study to compare 3 dimensional  Cone Beam Computed Tomography  CBCT  images of the patients that  have unilaterally impacted canines and to specify essential factors for  prediction of maxillary canine impactions. A total of 65 patients aged  between 9.6 and 13.8 years were included. Different variables were  extracted from radiographic images  including canine rotation  canine  crown position  canine angle to the midline  canine cusp tip to the  midline and to the occlusal plane  and canine angle to the lateral incisor.  In addition  angular and linear measurements were used to compare  impacted and non impacted canines. For statistical analysis  measure   ments  and scores of impacted and nonimpacted canines were compared  using the Fisher exact test and Mann Whitney U test. In addition  LR was  applied in the different variables  giving an 0.964 AUC value. Lastly  the  canine cusp tip to the occlusal plane  canine angle to the lateral incisor  and canine crown position were obtained  giving promising results  based on CBCT images and were the relevant predictors to this model.  Having said that  CBCT images are not readily available within dental  and orthodontic practices  and panoramic X rays diagnosis is considered  a more plausible diagnostic tool.  In another research  Alqerban et al.  6  conducted a study to form a  prediction criterion of maxillary canine impaction using angular and  linear measurements from 116 panoramic radiographs of patients aged M. Aljabri et al.                                                                                                                                                                                                                                 

Informatics in Medicine Unlocked 30  2022  100918 4between 7 and 14 years with unilateral canine impaction. A comparison  of the parameters of impacted and non impacted canines was performed  using the Wilcoxon  Mann  Whitney test. A LR model was then used to  combine the three most discriminating parameters  canine to first pre  molar angle  canine cusp to midline distance  and canine cusp to  maxillary plane distance. After combining the three most discriminating  parameters  they reached a strength of discrimination of 0.97 AUC   sensitivity of 90   and specificity of 94 .  Margot et al.  23  proposed and validated a model aimed at pre  dicting maxillary canine impaction based on linear and angular mea  surements. They used 306 panoramic radiographs of patients with  unilateral or bilateral canine impaction taken between the ages of 7 and  14 years. The parameters used for the prediction of impaction analyzed  through the backward selection were the canine to midline angle  canine  to first premolar angle  canine cusp to midline distance  canine cusp to  maxillary plane distance  sector  and all pairwise interactions. LR was  used to build the prediction model  with AUC and five fold cross    validation used to measure the performance. The results indicated an  AUC of 0.783 and a cross validated AUC of 0.750. In addition  a cut off  point of 0.342 with a sensitivity of 0.800 and a specificity of 0.598 was  obtained. Furthermore  the validation of the model performance ach  ieved an AUC of 0.594.  Laurenziello et al.  24  aimed to evaluate the performance of  different machine learning algorithms to classify maxillary canine  impaction in panoramic and lateral head film radiographs. They also  investigated facial growth patterns and variables related to the canine  position to determine which variable is the main differentiator between  erupted and impacted canines. They divided the dataset of 218 radio   graphs of patients between the age of 9 and 10 into two sets  namely  training and validation sets. The training set consisted of 174 records   131 of which were erupted and 43 impacted. Several factors were  investigated  such as the angle between the Sella nasion line and the  mandibular line  the long axis of the upper and lower central incisors   the long axis of the canine and median interincisal line  and the distance  between the canine cusp and occlusal plane. It was concluded that the  main difference between erupted and impacted canines is the angle  between the canine and median interincisal line. Additionally  different  machine learning algorithms  such as RF  SVM  Neural Network   K Nearest Neighbors  KNN   Naive Bayes  NB  and LR  were applied to  classify radiographs as impacted or erupted. The best performing model  was RF  with an accuracy of 88.3 . The pattern of facial vertical growth  seems to be influential in determining the risk of maxillary canine  impaction.  Warford et al.  1  conducted a study to predict maxillary canine  impaction in its early stages using sectors and angular measurement  methods. They mainly measured the angle of the unerupted canine from  panoramic radiographs to investigate whether adding the angulation  measurement with the sector location would help predict impaction  more accurately than using only the sector location. Initially  from an  original 200 records  82 records that met certain criteria were included.  These included erupted first maxillary incisors and molars  unerupted  canines and premolars  patient  s age under 12 years. Moreover  data on  the sector location  angular measurement  age  sex  and the classifica   tion of impacted or nonimpacted canines were collected to form the  dataset. First  to measure the angulation  a line was drawn to connect the  top points of both condyles  which was assigned it as a reference line.  They then measured the formed mesial angle from the drawn reference  line and the long axis of the unerupted canine to be the angle concerned  to predict the impaction. Furthermore  they located the sector location  for the cusp tip of the unerupted canine. For the sectors location  the  definition proposed by Ericson and Kurol  7  was used  with the ad  justments applied by Lindauer et al.  25 . Eventually  they used LR to  predict the impaction from the angulation and the sector location. The  descriptive statistics applied showed that the angulation was greater for  the nonimpacted canines  with a mean of 75.12  and smaller for the  impacted canines  with a mean of 63.20. It also showed that sector 2 is the median sector for impacted canines  and sector 1 is the median sector  for nonimpacted canines. Finally  it showed that the correlation between  the sector location and impaction is 68   and the correlation between  angulation and impaction is 48   which means that the sector location  is more closely linked to the impaction. Furthermore  their results  indicated that sector location was a very important indicator in pre  dicting impaction  whereas angulation was not helpful in predicting the  occurrence of impaction  although it probably slightly helped to predict  impaction in sector 2. In addition  the impaction depended more on the  sector location  as 82  of impacted canines were located in sectors 2  3   and 4. Impacted canines were most likely to be located in sector 3  with  an 87  probability  and there was a 99  probability that they would be  located in sector 4.  Table 1 below shows a summary of papers on canine impaction.  To conclude  CNN is a widely used deep learning model in the  medical image processing and classification research field. It has shown  significant performance in various classification tasks  which encour   aged the authors to implement it for the classification task. It was also  observed that there is a limited number of research studies on canine  impaction. To the best of the author  s knowledge  there are no research  studies that have used DL models to classify impacted canines based on  the Yamamoto classification.  One paper that applied deep learning for canine impaction classifi   cation was found ahead of this study  which used a different method   features  and classes. Their model was used to classify images into two  labels  erupted or impacted. However  the main objective of this study is  to classify each dental radiographic image into two types  namely Type I  and Type II. In addition  this study aimed to build a deep learning model  that exceeds the accuracy of Laurenziello et al.  24   as they achieved  88  accuracy using the RF classification model. A larger dataset than  that used by them was acquired  which was sufficient to build an effi  cient model that exceeds their accuracy. Also  the dataset was not used  by any other research as it was collected and labeled exclusively for this  research.  2.Materials and methods  This section describes the methodology followed in this research   which consists of data collection  image preprocessing  and image  classification using CNN architectures. Fig. 3 below illustrates the main  steps of the methodology.  2.1. Dataset description  The dataset used in this research was acquired from the dental hos  pital at Imam Abdulrahman Bin Faisal University in Dammam Saudi  Arabia. The dataset consists of 2D panoramic radiographic images with  an exposure setting of 78 kVp  4 mA  an exposure time of 12 ms  and  dimensions of  30 cm  12 cm  from patients between the ages of 9 and  12. Each image in the dataset was examined by expert dentists and  labeled with its proper type of canine impaction  namely Type I or Type  II  as shown in Figs. 4 and 5 below. The original dataset consisted of 416  samples  282 of which represented Type I  and 134 of which represented  Type II. For experimental purposes  random under sampling was per  formed to obtain a balanced dataset  with a total of 268 images  equally  divided into 134 of each type  Type I and Type II. Fig. 6 below shows the  distribution of samples in each class before and after under sampling.  2.2. Image preprocessing  Preprocessing refers to all transformations and changes to the orig  inal data before it is fed to the classification model  26 . Several tech  niques can be used for preprocessing to enhance the learning process. In  this study  pixel rescaling  image resize  augmentation  and resolution  enhancement were applied. M. Aljabri et al.                                                                                                                                                                                                                                 

Informatics in Medicine Unlocked 30  2022  100918 52.2.1. Pixel rescaling  In this research  we applied pixel rescaling to all images. Pixel  rescaling is to multiply every pixel in the image by a rescaling factor   27 . Digital images are formed by pixels. Each pixel has a value in the  range  0 255 . Where 0 is black and 255 is white. These values are too  high for the model to process with a typical learning rate  so we rescale it  to lower values. In our research  we applied Pixel rescaling with a factor  of 1. 255 which will transform each pixel value from the range  0 255   to  0 1 . This rescaling technique is called pixel normalization and is  preferred for neural network models  28 . Moreover  applying pixel  rescaling will benefit in processing all images in the same manner  where  all images will contribute evenly to the learning process in the network   29 .  2.2.2. Image resizing  Resizing is to ensure that all images have the same size  they should  be either down scaled or up scaled  30 . Downscaling the image will  result in a more efficient processing model by reducing computational  complexity and memory consumption. In this research  image resizing  was applied by downscaling all images in the dataset to the size 224   224. 2.2.3. Augmentation  The dataset needs to be augmented to increase the variability and  generality of the neural network model by exposing it to more examples.  The idea behind data augmentation is to increase the size of the dataset  by applying some modifications to the original data. This can be done  through several techniques  such as rotation and flipping of images.  These techniques will result in more images being generated  which will  increase the size of the dataset. Such an incremental increase in the  dataset size will expose the model to a wider variety of images  31 32 .  In this research  several augmentation techniques were applied  namely   horizontal flip  zoom  and shear to each image in the dataset. A zoom  range of 0.2 and a shear range of 0.2 were applied.  2.2.4. Image resolution enhancement  Since the resolution of the images in the dataset was quite low  the  resolution was enhanced using an online tool called  Let s Enhance     33 . This is an online paid service that automatically increases the  resolution of the images using artificial intelligence by uploading the  images on the website. Fig. 7a below demonstrates an image before  enhancement and Fig. 7b demonstrates the image after applying image  resolution enhancement. Table 1  Summary of papers on canine impaction.   Study Main Objective Dataset Method Algorithm Results   1  Predict the maxillary canine  impaction using sectors and  angular measurement methods. 82 panoramic  radiographs. Applied some descriptive statistics and  calculated the zero order correlation  between the sector location and the  impaction and between the angulation and  the impaction. LR. The impaction depended more  on the sector location. With  most likelihood for impacted  canines to be located in sector 4  with 99  probability.   6  To form a prediction criterion of  the maxillary canine impaction. 116 radiographs of  patients with unilateral  canine impaction   aged  7 14 years. . Comparison of parameters of impacted and  non impacted canines using the  Wilcoxon  Mann  Whitney test. LR model to combine  the three most  discriminating  parameters. Strength of discrimination   0.97 AUC  0.90. Sensitivity   0.90. Specificity  0.94.   22  Compare 3 dimensional cone beam  computed tomography  CBCT   images of the patients that have  unilaterally impacted canines and  to specify essential factors for  prediction of maxillary canine  impactions. CBCT images of 65  patients  aged   9.6 13.8 years . A statistical analysis was used to compare  between impacted and non impacted  canines. Statical analysis   Fisher exact and  Mann Whitney U  tests  and LR. AUC  0.964.   23  A prediction model for maxillary  canine impaction based on linear  and angular measurements. Panoramic radiographs  of unilateral or  bilateral canine  impaction of 306  patients  aged  7 14  years . The parameters were analyzed through the  backward selection procedure. LR. AUC  0.783.  Cross validated AUC  0.75.  Cut off point  0.342.  Sensitivity  0.800. Specificity   0.598. Validation AUC  0.594.   24  Evaluate the performance of  different machine learning  algorithms to classify the maxillary  canine in panoramic and lateral  head film radiographs. 218 radiographs of  patients  aged  9 10  years . Investigated the correlation between several  variables in impacted and erupted canines  by calculating the Spearman rank  correlation coefficient. RF  SVM  Neural  Network  KNN  NB  and LR. Accuracy  0.883.   Fig. 3.Methodology steps.  M. Aljabri et al.                                                                                                                                                                                                                                 

Informatics in Medicine Unlocked 30  2022  100918 6Finally  using a CNN model does not require intensive preprocessing   as it can observe fine grained patterns efficiently. Many medical appli   cations and models are built using CNN without applying preprocessing.  Some have high accuracy  while others stated that the reason for their  low accuracy is that they did not apply preprocessing. Since the model  s  accuracy is highly dependent on the quality and quantity of the dataset   preprocessing needs to be considered. It must also be considered to  reduce computational complexity and improve the learning process. 2.3. Image classification  Convolutional Neural Networks  CNN  are widely used and suc  cessful in image classification tasks due to their high level representa   tions  which are composed of two main parts  namely feature extraction  and classification. In the feature extraction part  features are detected  after the network performs a set of convolutions and pooling operations.  Where the model takes an image as an input and output a set of features  that will be fed to the classification part. There are five main steps to  building a CNN. The first step is convolution  where the model obtains  many feature maps using a filter or a kernel. The second step is the  Rectified Linear Unit  ReLU   which is applied to increase non linearity  in the network because the images themselves are non linear. The third  step is pooling  also referred to as down sampling  which operates on  each feature map independently. Between max pooling  min pooling   sum pooling  and mean pooling  the most common approach applied is  max pooling  which takes the maximum pixel value in a segment.  Pooling helps prevent overfitting by reducing spatial size and parame   ters. Overfitting is a situation where the model achieves high accuracy  with the training set  but a relatively low accuracy with the test set. The  fourth step is flattening  which is applied to a pooled feature map to  produce a long vector  a single column   to later use it as an input to an  Artificial Neural Network  ANN . The fifth step is the full connection   where a whole ANN is added to the CNN. The main objective of an ANN  is to combine the features into more attributes that better predict the  classes. Error calculation and backpropagation occur in this step  where  weights and feature detectors are adjusted accordingly. Forward prop  agation and backpropagation continue through many stages  which is  how the network is optimized and trained. Once the network is fully  trained  an image can be inputted into the model  which will be able to  recognize it and predict a class. After the construction of the CNN  building blocks  model training  testing  and evaluation are performed.  The model is trained using the training set and their classes. Once the  model is trained  it is tested using the testing set  which has not been  seen by the model. Accordingly  the accuracy will be evaluated. Finally   the model can be used to classify new images from the real world  34   35 . Fig. 8 below illustrates the building blocks of a CNN image classi   fication model.  2.3.1. Convolutional neural network  CNN  architectures  To take advantage of the power of CNN  a large dataset  computa   tional power  and time are required  which can be costly. A common  solution to overcome these challenges and to perform deep learning  using CNN models on smaller datasets while maintaining high perfor   mance and results  is transfer learning  which reuses the knowledge of a  related task from a pre trained model  36 . The pre trained CNN ar  chitectures utilized in this study  DenseNet  VGG  Inception V3  and  ResNet 50  are discussed below.  Fig. 4.Type I.   Fig. 5.Type II.   Fig. 6.Representation of samples.  M. Aljabri et al.                                                                                                                                                                                                                                 

Informatics in Medicine Unlocked 30  2022  100918 72.3.1.1. DenseNet. The densely connected convolutional neural  network  DenseNet  is a CNN architecture that connects layers directly  to each other to ensure maximum information flow. In DenseNest  the  input to a layer is the concatenation of inputs of all previous layers. This  means that each layer takes the feature maps from all preceding layers.  DenseNet is parameter efficient and requires fewer parameters to pre  serve information since all information is concatenated. DenseNet is  divided into dense blocks that contain some layers. These layers  generate feature maps with equal dimensions to make concatenation  possible. Moreover  DenseNet contains transition layers. These layers  are used to aggregate feature maps of a dense block and reduce their  dimensionality before they enter the next dense block  37 .  2.3.1.2. VGG. VGG is a convolutional neural network  CNN  model  used for image recognition. The input size used in VGG model is a 224   224 pixel RGB image. The receptive fields use is 3  3 and incorporates  1  1 convolutional layers. VGG contain three fully connected layers at  the top of the network. Hidden layers in VGG use ReLU to reduce  training time. Moreover  Local Response Normalization  LRN  is not  generally used in VGG. The two types mainly used in VGG are VGG16  and VGG19. VGG16 contains 16 layers and is usually used. On the other hand  VGG19 contains 19 layers and is slightly better because it contains  more memory usage. VGG has a small size convolution filter. This leads  to many weight layers  which will consequently improve performance   38 .  2.3.1.3. Inception V3.GoogLeNet  Inception V3  is a CNN architecture  that was developed by Google in 2014. It provides deeper convolutions  compared to previous architectures  as it consists of 22 layers that  enhance the performance of the network. It showed promising results  as  it decreased the error rate by a significant margin compared to AlexNet  and VGG architectures. What makes the architecture more advanced is  its efficiency in terms of memory usage and computational cost. The  architecture is designed to efficiently perform accurate image classifi   cation even in individual and low computational resource devices  39 .  It concatenates three types of convolutions  such as 1  1  3  3  and 5   5. The concatenation of all three types of convolutions is beneficial as  images are preprocessed at multiple scales  which increases the power of  the neural network. It also applies a 1  1 dimensionality reduction  whenever computation requirements increase  which reduces compu   tational power consumption. Moreover  the last layer of the network in  the architecture is not fully connected as in previous architectures.  Fig. 7.This figure demonstrates the image before and after resolution enhancement   a  before resolution enhancement   b  after resolution enhancement.   Fig. 8.The building blocks of a CNN model.  M. Aljabri et al.                                                                                                                                                                                                                                 

Informatics in Medicine Unlocked 30  2022  100918 8Instead  it uses a method called Global Average Pooling. This method is  used to overcome the problem of over fitting because it has no param   eter to optimize  which makes the interpretation of features easier.  2.3.1.4. ResNet 50. ResNet 50 is a CNN model that has a deep structure  used in image classification. It has 50 layers and more than 23 million  trainable parameters. It utilized the idea of  shortcut connection   which  skips three layers  40 . The ResNet 50 architecture consists of five  stages. Each stage has a convolution and identity block  and all the  convolution block and identity block have three convolution layers  41 .  2.4. Model evaluation  The performance of each CNN architecture was evaluated using five  standard evaluation metrics  namely accuracy  1   precision  2   recall   3   specificity  4   and F score  5 . The accuracy represents the pro  portion of correctly classified images in relation to the total number of  classified images. The precision represents the number of positive  samples correctly labeled as positive over the total number of samples  labeled as positive. The recall represents the number of positive samples  correctly labeled as positive over the total number of positive samples.  The specificity represents the number of negative samples correctly  labeled as negative over the total number of negative samples. The F   Score represents the weighted harmonic average of recall and precision   42 . The architecture that achieves the highest accuracy is selected to  classify new radiographic images. The performance metrics are calcu   lated as follows  in which TP  TN  FP  and FN represent True Positive   True Negative  False Positive  and False Negative respectively   Accuracy TP TN TP FN TN FP 1    Precision TP TP FP 2    Recall TP TP FN 3    Specificity TN TN FP 4    F Score 2 precision recall precision recall 5    3.Experimental setup and results  For this research  two experiments were conducted  the first without  balancing the number of samples  whereas the second was with a  balanced number of samples in each class. Each experiment was parti  tioned into a 20  test set and 80  training set. In all experiments  the  code was executed on a GPU in Google Collaboratory  43  and was  implemented using the Python programming language  utilizing Ten  sorFlow and Keras libraries. All the experiments were done on Mac OS X  and Windows 10 with an 8 GB RAM and a 256 GB hard drive capacity.  Also  all the experiments were conducted on the following parameters  settings  150 epochs  8 batch size  and 224 image shape.  3.1. Experiment 1  In this experiment  canine impaction classification was applied using  the four CNN architectures. The whole dataset was utilized  which  consists of 416 samples  282 of which represent Type I and 134 Type II.  The resolution of the dataset was enhanced  along with applying data  augmentation and image resize. This experiment was applied to four  architectures  namely DenseNet 121  VGG 16  Inception V3  and  ResNet 50  which achieved an accuracy of 0.7976  0.6548  0.8095  and  0.7619 respectively. Table 2 below demonstrates the results of all the four models in this experiment and shows that Inception V3 was the  best performing model.  3.2. Experiment 2  In this experiment  canine impaction classification was applied using  the four CNN architectures. Data balancing was performed to produce a  total of 268 images  equally divided into 134 of each type  namely Type I  and Type II. Image resolution enhancement on the dataset was applied  as a preprocessing technique along with augmentation and image resize.  This experiment was applied to four architectures  namely Dense Net   121  VGG 16  Inception V3  and ResNet 50  with an accuracy of  0.6852  0.5741  0.9259  and 0.8704 respectively. Table 3 below shows  the results of all the four models in this experiment and shows that  Inception V3 was the best performing model.  4.Discussion  In this research  two different experiments were carried out for  comparison purposes. In the first experiment  the whole unbalanced  dataset was used after resolution enhancement. All four models in this  experiment did not perform well  as the highest accuracy achieved was  0.8095. In addition  since the dataset is not balanced  the results of this  experiment appeared to be inaccurate  as shown earlier in Table 2  the  difference between precision and specificity for each model was huge   which indicated inaccuracy. Such a gap between precision and speci   ficity showed that the models could not predict both classes properly due  to data imbalance and bias. Moreover  building a model that classifies  such a critical task requires higher rates of accuracy and precision.  Therefore  it was decided to balance the dataset to get a clearer insight  into the actual performance of the models and to avoid bias. In this  experiment  the performance of Inception V3 and ResNet 50 was better  than in the first experiment  whereas DenseNet 121 and VGG 16 showed  poorer performance. Furthermore  the gap difference between precision  and specificity was reasonable  indicating that the results were more  reliable than the first experiment. Excluding VGG 16 and DensenNet   121  this experiment produced better results  and achieved an accu  racy of 0.9259 using Inception V3  which was the best outcome.  To conclude  the Inception V3 model outperformed the other models  in all experiments. Furthermore  Inception V3 achieved the highest ac  curacy 0.9259  using a balanced dataset with image resolution  enhancement. Although deep learning models should be tested using a  substantial number of samples  ResNet 50 and Inception V3 appeared to  have potentially impressive performance given the fact that the dataset  was small  and the task was complicated due to the difficulty in dis  tinguishing the difference between Type I and Type II. In contrast   DenseNet 121 and VGG 16 exhibited lower performance in all the ex  periments related to canine impaction classification. It is worth  mentioning that these models were trained and evaluated on two types  of canine impaction instead of seven types of Yamamoto classification  due to the lack of radiographs of the other canine impaction types.  5.Conclusions  Two experiments were conducted to assess the efficiency of deep  learning models to classify the type of canine impaction from panoramic  dental radiographic images. In addition  several CNN architectures were  Table 2  Results of the four models in experiment 1.    Accuracy Precision Recall Specificity F Score  Inception V3 0.8095 0.7143 0.4545 0.9355 0.5555  ResNet 50 0.7619 0.8249 0.7780 0.9216 0.8008  DenseNet 121 0.7976 0.7825 0.6219 0.9178 0.6930  VGG 16 0.6548 0.1000 0.0167 0.9305 0.0286  M. Aljabri et al.                                                                                                                                                                                                                                 

Informatics in Medicine Unlocked 30  2022  100918 9implemented to produce the most reliable classification model with the  highest possible accuracy. The main contributions of this study include    Created an efficient deep learning model with high accuracy  preci   sion  recall  specificity  and f score of 0.9259  0.9355  0.9355   0.9130  and 0.9355  respectively for canine impaction classification  from panoramic dental radiographs.    Covered the gap of the limited number of automated machines for  canine impaction classification using deep learning.    Compared different CNN architectures in terms of their performance  in the canine impaction classification task.  One pivotal limitation of this work is the small size of the labeled  dataset. The obtained radiographic images also suffered from low res  olution and quality  which required more data preprocessing.  As a future work  we recommend a more extended dataset of images  to train and test the model. Furthermore  more CNN architectures can be  implemented in future research to improve and enhance the canine  impaction classification results. In addition  the proposed model could  be applied and evaluated with a dataset that contains all seven types of  canine impaction.  Funding  This work was supported by the Deanship of Scientific Research   Imam Abdulrahman Bin Faisal University  Dammam  Saudi Arabia   Grant No. 2021 175 CSIT .  Declaration of competing interest  The authors declare that they have no known competing financial  interests or personal relationships that could have appeared to influence  the work reported in this paper.  Acknowledgments  We would like to thank Dental Hospital at Imam Abdulrahman Bin  Faisal University for their cooperation on providing the dataset. We also  extend our appreciation to the Deanship of Scientific Research  Imam  Abdulrahman Bin Faisal University for funding this project.  References   1 Warford JH  Grandhi RK  Tira DE. Prediction of maxillary canine impaction using  sectors and angular measurement. Am J Orthod Dentofacial Orthop 2003 124 6    651 5. https   doi.org 10.1016 S0889 5406 03 00621 8 .   2 Alamri A  Alshahrani N  Al Madani A  Shahin S  Nazir M. Prevalence of impacted  teeth in saudi patients attending dental clinics in the eastern province of Saudi  Arabia  a radiographic retrospective study    the. 2020 Sci World J 2020 1  6.  https   doi.org 10.1155 2020 8104904 .   3 Bedoya M  Park J. A review of the diagnosis and management of impacted  maxillary canines. J Am Dent Assoc 2009 140 12  1485  93. https   doi.org   10.14219 JADA.ARCHIVE.2009.0099 .   4 Manne R  Gandikota C  Juvvadi SR  Rama HRM  Anche S. Impacted canines   etiology  diagnosis  and orthodontic management. J Pharm BioAllied Sci 2012 4   6  234. https   doi.org 10.4103 0975 7406.100216 .   5 Aslan BI     nc  N. Clinical consideration and management of impacted maxillary  canine teeth. In  Virdi MS  editor. Emerging trends in oral health sciences and  dentistry. IntechOpen  2015. https   doi.org 10.5772 59324  Online  Available   https   www.intechopen.com chapters 47825 .  6 Alqerban A  Storms A  Voet M  Fieuws S  Willems G. Early prediction of maxillary  canine impaction. Dentomaxill Radiol 2016 45 3  20150232. https   doi.org   10.1259 DMFR.20150232 .   7 Ericson S  Kurol J. Early treatment of palatally erupting maxillary canines by  extraction of the primary canines. EJO  Eur J Orthod  1988 10 4  283  95. https     doi.org 10.1093 EJO 10.4.283 .   8 Kumar S  Mehrotra P  Bhagchandani J  Singh A  Garg A  et al. Localization of  impacted canines. J Clin Diagn Res 2015 9 1  11  4. https   doi.org 10.7860   JCDR 2015 10529.5480 .   9 Yamamoto G  Ohta Y  Tsuda Y  Tanaka A  Nishikawa M  et al. A new classification  of impacted canines and second premolars using orthopantomography. Asian J  Oral Maxillofac Surg 2003 15 1  31  7. https   doi.org 10.1016 S0915 6992 03   80029 8 .   10  Kim KG. Book review  deep learning. Healthcare Inform Res 2016 22 4  351  4.  https   doi.org 10.4258 HIR.2016.22.4.351 .   11  Goodfellow I  Bengio Y  Courville A. Deep learning learning. Cambridge  Mass  The  MIT Press  2016  Online . Available  https   mitpress.mit.edu books deep   learning .   12  Schwendicke F  Golla T  Dreher M  Krois J. Convolutional neural networks for  dental image diagnostics  a scoping review. J Dent 2019 91 103226. https   doi.  org 10.1016 J.JDENT.2019.103226 .   13  Hwang J  Jung Y  Cho B  Heo M. An overview of deep learning in the field of  dentistry. Imag Sci Dentis 2019 49 1  1  7. https   doi.org 10.5624   ISD.2019.49.1.1 .   14  What Is Deep Learning    How It Works  Techniques   Applications   MATLAB    Simulink.   https   www.mathworks.com discovery deep learning.html  accessed  Aug. 07  2021 .   15  Deep Learning Definition  Artificial Intelligence .   https   www.investopedia.  com terms d deep learning.asp  accessed Aug. 07  2021 .   16  A Comprehensive Guide to Convolutional Neural Networks   the ELI5 way   by  Sumit Saha   Towards Data Science.   https   towardsdatascience.com a compre  hensive guide to convolutional neural networks the eli5 way 3bd2b1164a53   accessed Aug. 07  2021 .   17  Kuwada C  Ariji Y  Fukuda M  Kies Y  Fujita H  et al. Deep learning systems for  detecting and classifying the presence of impacted supernumerary teeth in the  maxillary incisor region on panoramic radiographs. Oral Surg Oral Med Oral  Pathol Oral Radiol 2020 130 4  464  9. https   doi.org 10.1016 J.  OOOO.2020.04.813 .   18  Minnema J  Eijnatten M  Hendriksen AA  Liberton N  Pelt DM  et al. Segmentation  of dental cone beam CT scans affected by metal artifacts using a mixed scale dense  convolutional neural network. Med Phys 2019 46 11  5027  35. https   doi.org   10.1002 MP.13793 .   19  Ngoc VTN  Agwu AC  Son LH  Tuan TM  Giap CN  et al. The combination of  adaptive convolutional neural network and bag of visual words in automatic  diagnosis of third molar complications on dental x ray images. Diagnostics 2020 10   4  209. https   doi.org 10.3390 DIAGNOSTICS10040209 .   20  Lee J  Kim D  Jeong S  Choi S. Detection and diagnosis of dental caries using a deep  learning based convolutional neural network algorithm. J Dent 2018 77 106  11.  https   doi.org 10.1016 J.JDENT.2018.07.015 .   21  Lee K  Jung S  Ryu J  Shin S  Choi J. Evaluation of transfer learning with deep  convolutional neural networks for screening osteoporosis in dental panoramic  radiographs. J Clin Med 2020 9 2  392. https   doi.org 10.3390 JCM9020392 .   22  Alqerban A  Jacobs R  Fieuws S  Willems G. Radiographic predictors for maxillary  canine impaction. Am J Orthod Dentofacial Orthop 2015 147 3  345  54. https     doi.org 10.1016 J.AJODO.2014.11.018 .   23  Margot R  Maria CL  Ali A  Annouschka L  Anna V  et al. Prediction of maxillary  canine impaction based on panoramic radiographs. Clinic Exper Dental Res 2020 6   1  44  50. https   doi.org 10.1002 CRE2.246 .   24  Laurenziello M  Montaruli G  Gallo C  Tepedino M  Guida L  et al. Determinants of  maxillary canine impaction  retrospective clinical and radiographic study. J Clinic  Exper Dentis 2017 9 11  e1304  9. https   doi.org 10.4317 JCED.54095 .   25  Lindauer SJ  Rubenstein LK  Hang WM  Andersen WC  Isaacson RJ. Canine  impaction identified early with panoramic radiographs. J Am Dent Assoc 1992 123   3  91  7. https   doi.org 10.14219 JADA.ARCHIVE.1992.0069 .   26  Preprocessing for deep learning  from covariance matrix to image whitening.  accessed Aug. 08  2021  https   www.freecodecamp.org news https medium com   hadrienj preprocessing for deep learning 9e2b9c75165c  .   27  tf.keras.preprocessing.image.ImageDataGenerator.   https   www.tensorflow.  org api docs python tf keras preprocessing image ImageDataGenerator   accessed Sep. 22  2021 .   28  How to Normalize  Center  and Standardize Image Pixels in Keras. https   machine  learningmastery.com how to normalize center and standardize images with the   imagedatagenerator in keras  . accessed Sep. 22  2021.   29   Keras Image Preprocessing  scaling image pixels for training.   https   www.linked  in.com pulse keras image preprocessing scaling pixels training adwin jahn    accessed Sep. 22  2021 .   30  Image Pre processing. In this article  we are going to go   by Prince Canuma    Medium.   https   prince canuma.medium.com image pre processing c1aec  0be3edf  accessed Aug. 29  2021 .   31  Image Augmentation for Convolutional Neural Networks   by ODSC   Open Data  Science   Medium.   https   medium.com  ODSC image augmentation for convo  lutional neural networks 18319e1291c  accessed Aug. 29  2021 .   32  Image Data Pre Processing for Neural Networks   by Nikhil B   Becoming Human   Artificial Intelligence Magazine.   https   becominghuman.ai image data pre pr  ocessing for neural networks 498289068258  accessed Aug. 29  2021 . Table 3  Results of the four models in experiment 2.    Accuracy Precision Recall Specificity F Score  Inception V3 0.9259 0.9355 0.9355 0.9130 0.9355  ResNet 50 0.8704 0.8562 0.9085 0.8475 0.8816  DenseNet 121 0.6852 0.7823 0.6674 0.8143 0.7203  VGG 16 0.5741 0.4931 0.3339 0.6567 0.3981  M. Aljabri et al.                                                                                                                                                                                                                                 

Informatics in Medicine Unlocked 30  2022  100918 10 33  Let s Enhance   free online image upscale and enhancement. Try neural networks  to increase resolution and quality now    Let s Enhance. accessed Aug. 08  2021   https   letsenhance.io .   34  The Complete Beginner s Guide to Deep Learning  Convolutional Neural Networks  and Image Classification   by Anne Bonner   Towards Data Science.  https   towa  rdsdatascience.com wtf is image classification 8e78a8235acb  accessed Aug. 30   2021 .   35  Convolutional Neural Networks  CNN  explained step by step   Medium.   https   medium.com analytics vidhya convolutional neural  networks cnn explai  ned step by step 69137a54e5e7  accessed Aug. 30  2021 .   36  Han D  Liu Q  Fan W. A new image classification method using CNN transfer  learning and web data augmentation. Expert Syst Appl 2018 95 43 56. https   doi.  org 10.1016 J.ESWA.2017.11.028.   37  Understanding and visualizing DenseNets   by Pablo Ruiz   Towards Data Science.   https   towardsdatascience.com understanding and visualizing densenets 7f6  88092391a  accessed Aug. 08  2021 .  38  VGG Neural Networks  The Next Step After AlexNet   by Jerry Wei   Towards Data  Science.  https   towardsdatascience.com vgg neural networks the next step aft  er alexnet 3f91fa9ffe2c  accessed Aug. 08  2021 .   39  Understanding GoogLeNet Model   CNN Architecture   GeeksforGeeks. accessed  Aug. 08  2021  https   www.geeksforgeeks.org understanding googlenet mode  l cnn architecture .   40  Understanding ResNet50 architecture. accessed Aug. 08  2021  https   iq.opengen  us.org resnet50 architecture .   41  Understanding and Coding a ResNet in Keras   by Priya Dwivedi   Towards Data  Science.  https   towardsdatascience.com understanding and coding a resnet in   keras 446d7ff84d33  accessed Aug. 08  2021 .   42  Accuracy  Recall  Precision  F Score   Specificity  which to optimize on    by  Salma Ghoneim   Towards Data Science.  https   towardsdatascience.com   accuracy recall precision f score specificity which to optimize on 867d3f11124   accessed Sep. 03  2021 .   43  Welcome To Colaboratory   Colaboratory.  https   colab.research.google.com no  tebooks intro.ipynb  accessed Sep. 22  2021 . M. Aljabri et al.                                                                                                                                                                                                                                 

