Informatics in Medicine Unlocked 30  2022  100923 Available online 23 March 2022 2352 9148   2022 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY NC ND license   http   creativecommons.org licenses by  nc nd 4.0   .Robust multi modal prostate cancer classification via feature autoencoder  and dual attention  Bochong Lia    Ryo Oka  M.Db  Ping Xuan  Professorc  Yuichiro Yoshimura  PhDd   Toshiya Nakaguchi  Professore  aGraduate School of Science and Technology  Chiba University  1 33  Yayoicho  Inage Ward  Chiba shi  263 002  Japan  bToho University Sakura Medical Center  564 1 Shimoshizu  Sakura  Chiba  Sakura  285 0841  Japan  cSchool of Computer Science and Technology  Heilongjiang University  74 Xuefu Road  Nangang District  Harbin  Harbin  150080  China  dToyama University School of Medicine  3190 Gofuku  Toyama  Gofuku  930 8555  Japan  eCenter for Frontier Medical Engineering  Chiba University  1 33  Yayoicho  Inage Ward  Chiba shi  Chiba  263 8522  Japan    ARTICLE INFO   Keywords   Prostate cancer  Computer aided detection  Magnetic resonance imaging  Machine learning ABSTRACT   Prostate cancer is the second leading cause of cancer death in men. At present  the methods for classifying early  cancer grades on MRI images are mainly focused on single image modality and with low robustness. Therefore   this paper focuses on exploring the method of classifying cancer grades on multi modality MRI images and  maintaining robustness. In this paper  we propose a novel and effective multi modal convolutional neural  network for discriminating prostate cancer clinical severity grade  i.e.  Robust Multi modal Feature Autoencoder  Attention net  RMANet   this model greatly improves the accuracy and robustness of the model. T2 weighted and  Diffusion weighted imaging are used in this article. The model consists of two branches  one of them is to learn  the overall features of two MRI modalities by building a ten layer CNN network with two input shared weights   and the other branch uses auto encoder structure with classical U net as the backbone to learn specific features of  each modality and to improve the robustness of the classification model. In the branch of learning overall fea  tures of each modality  the novel dual attention mechanism is added to this branch  through which the attention  mechanism can better direct the learning focus of the model to the cancerous regions. Experiments were con  ducted on the ProstateX dataset and augmented with hospital data. By comparing with other baseline methods   multi modal input methods  and State of the Art  SOTA  methods  the AUC values obtained by the proposed  model  reaching 0.84  in this paper after the test set are higher than other classical models and most recent  methods  and the sensitivity values  reaching 0.84  are higher than the recent method.    1.Introduction  Prostate cancer is the most prevalent cancer among men worldwide  and is the second leading cause of cancer death in men  1 . We generally  use the Gleason score  GS   2 to discriminate between the clinical  severity of prostate cancer  doctors consider a prostate cancer score of  less than or equal to 6 as a low clinical risk and a score of greater than or  equal to 7 as a high clinical risk. In conventional diagnosis  there are two  main ways to confirm the diagnosis of prostate cancer  and one is  Prostate Specific Antigen  PSA   3   PSA is tissue specific and is only  found in the cytoplasm of human prostate alveoli and ductal epithelial  cells and is not expressed in other cells. However  it is not tumor specific.  Prostatitis  benign prostatic hyperplasia and prostate cancer can all result in increased total PSA levels  free PSA plus complex PSA . The  other approach for diagnosis is a rectal  4   of which the main purpose is  to find out the shape  size and hardness of the prostate  whether the  surface is smooth  whether there are nodules and pressure pain  whether  the central sulcus is present  shallow or absent  whether the gland is  fixed  whether there is a twisting sensation on palpation  and so on. Also   rectal is used to clarify the condition of the anal sphincter  rectum and  seminal vesicles. However  the specificity of PSA and the sensitivity of  rectal are both very low. Therefore  we require a non invasive method  with superior sensitivity and specificity to assist physicians in diag  nosing the clinical severity of prostate cancer.  In recent years  prostate cancers are increasingly being diagnosed by  Magnetic Resonance Imaging  MRI  images. MRI  5  is one of the few   Corresponding author.  E mail address  li.bochong chiba u.jp  B. Li .   Contents lists available at ScienceDirect  Informatics in Medicine Unlocked  u   zkw  s yo kr o     1ow  o to 1m y2w m k o2ty   https   doi.org 10.1016 j.imu.2022.100923  Received 31 January 2022  Received in revised form 13 March 2022  Accepted 18 March 2022   

Informatics in Medicine Unlocked 30  2022  100923 2safe  fast and accurate clinical diagnostic methods that does not cause  any harm to the human body and has excellent resolution of prostate  tissue. The measurement of tumor induced tissue changes relies on  complementary biological information provided in multiple MRI mo  dalities  i.e.  T1 weighted image  T1   T2 weighted  T2   and  Diffusion weighted Imaging  DWI  which includes ADC images calcu   lated from altered b values  6 .  Computer aided diagnostic  CAD   7  techniques based on machine  learning methods have been used extensively for the prediction and  detection of early prostate cancer  8 10 . These machine learning based  methods are divided into two main steps  feature learning and classifi   cation. During the feature learning phase  if many non important fea  tures are learned  these features can be reduced or selected before the  classification phase. Several machine learning methods are now avail  able to classify the clinical severity of prostate cancer. However  these  methods frequently require cropping or manual partitioning of the  prostate in order to focus the model on the prostate region  which in  creases the physician  s workload.  On top of machine learning  new techniques such as deep learning   11  have emerged in the last few years. Deep learning achieves stun  ning performance in image processing  12 15 and natural language  processing tasks. This new technology is already being used in CAD and  has achieved impressive performance in tasks such as medical image  classification  16   detection  17   segmentation and alignment.  Recently  there has been an in crease in the use of deep learning based  CAD techniques for medical imaging data such as MRI  18  and CT   19   and these techniques have been used to solve the problem of early  prostate cancer prediction  20 . The use of deep learning for classifi   cation of prostate cancer can be divided into two categories based on the  model  transfer learning  and classical convolutional neural networks.  As deep neural networks are used in an increasing number of domains   transfer learning  21 22  has become a very popular technique for  developing deep learning models. In some specific objectives  it is often  difficult for us to get enough data sets for feature extraction  so we have  to use the power of pre trained models including VGG16  13   VGG19   Resnet50  and Resnet101  15 .  In MRI images  we generally obtain images of different modalities  through different examination steps  such as T2  DWI  and DCE  23 . In  T2 im ages  the normal peripheral zone of the prostate shows high  signal  and the central zone and periurethral gland area show low T2  signal. Therefore  the various zones of the prostate are easily distin   guished on T2. DWI im ages are used to detect the degree of restricted  diffusive movement of water molecules in tissues. The degree of diffu  sion of water molecules can provide information about the tissues to  identify benign and malignant cancers. In DCE  24 images  the contrast  agent  CA  is injected into the vein at a con stant rate to increase the  signal intensity of the image. The CA enters the capillaries from the  arteries and extravasates into the extracellular space to obtain the  perfusion and permeability of the tissue.  From the perspective of MRI image modality  these CAD methods can  be divided into two categories  namely those based on unimodal MRI  and multimodal MRI. Both methods achieve stunning performance in a  variety of medical image targets. In a single modal approach  previous  methods use only one modality of images for learning  but the limited  features of single modality images  for example  in some patients   im  ages where the features of the target region are not obvious  it can lead  to results that will not be the best. There are some limitations to using  unimodal images  25   multimodal learning convolutional neural net  works can learn features more comprehensively and avoid unimodal  feature homogeneity  23 . These articles combine different modalities  of MRI images for learning  9 26       29 . And in this study  41   the  authors summarize the latest advances in the use of deep learning  techniques for prostate cancer diagnosis. The latest research on prostate  cancer using transfer learning techniques on DWI can also be found in  this article  42   The authors used advanced transfer learning methods  to investigate the effect of different b values in DWI on the results. In order to exploit the features of different modal images in MRI  images and to complement the features of different modalities with each  other  we propose a novel robust deep learning network model to predict  early prostate cancer grade. We divide the model into two branches  one  branch that learns overall features of two MRI modalities and another  branch that learns specific features of each modality. To enable the  model to better focus on learning features of cancerous regions  we add  position attention and channel attention mechanisms to the network.  And the difference from the previous method is that we do not require  the physician to segment the prostate area in the image pre processing  stage. According to Lalken  s study  45   for early cancer screening   high sensitivity is more important to physicians and patients than  specificity  and our method achieves a higher sensitivity than other  methods. The main contributions of our research are mainly in the  following aspects    1  We propose a novel multimodal integration of deep learning for  classification of prostate cancer clinical severity  which differs from  the backbone approach in traditional methods by using a more  streamlined feature decoder.   2  We use autoencoders for the first time in a multimodal classification  network and combine a novel robust multimodal attention network  to im prove the accuracy of early prostate cancer prediction.   3  We incorporate position and channel attention mechanisms into the  model to improve the accuracy and robustness of the model.  The rest of the paper is structured as follows. The following section  will focus on the proposed method and the dataset used for the experi   ments  the third section presents the experimental results and compares  the baseline with the latest methods  the discussion is in the fourth  section  and then the conclusions are in the fifth section.  2.Method  2.1. RMANet  In this paper  we propose a novel convolutional neural network   CNN  architecture  i.e.  Robust multi modal feature autoencoder  attention net  RMANet   which is robust to missing modalities  as shown  in Fig. 1. Our network disentangles multi modal features into a  modality specific appearance part and modality invariant content part.  Based on past studies  30 32   the modality invariant feature part of  each modality is fused into a shared representation containing  discriminative information for classification tasks. To enhance the  modality invariant feature  the shared representation is required to  reconstruct any modality given a corresponding appearance code  even  in the absence of some modality ies . Furthermore  we employ a novel  channel and region attention module to enable the model to focus on  learning features of the cancerous region  as shown in Fig. 1  we  represent multi modal images by  x1C Cxm   in this article we set M to 2.  The whole RMANet is mainly divided into three parts  the feature  extractor and classifier is the first part Content code  the second part is  an autoencoder with U net structure to assist the model to improve the  learning of the features of each modality which is the appearance code   the third part is the dual attention module  we add the dual attention  module to the back of the first part feature extractor  the dual attention  module can enable the model to emphasize more on the features of the  prostate region when extracting the features.  In order to improve the feature learning effect and increase the  robustness of the model  we use the auto encoder. The current model  uses a lot of multi input models  but it rarely the limited input modality  is used to maximize the feature learning  while the advantage of the  auto encoder is that it has the effect of dimensionality reduction  it can  extract the main features  it also can capture the main features  and has a  certain degree of noise immunity  so to a certain extent  it can avoid too  much noise in the image to affect the process of learning features  so that B. Li et al.                                                                                                                                                                                                                                        

Informatics in Medicine Unlocked 30  2022  100923 3the learning process of the model has been focused on the key features.  We use an autoencoder to help the content encoding part to solve some  components that affect the model learning result because of irrelevant  information in the image. Each modal xi is fed into the respective  appearance encoder Ea i and respective CNNu i   after CNNu i   the model can  get the content feature map.  Since in the direction of prostate cancer  all cancerous sites are in the  prostate region  we expect the model to automatically ignore irrelevant  information  irrelevant regions  in other words  we can also make the  model about the region where the cancer will occur or the cancerous  region  there has been a lot of attention mechanism model applied to  image processing in the previous work  which is characterized by low  number of parameters and better results. In order to increase the accu  racy of the model and to make the model focus better on the features in  the key areas  we use a dual attention model  just like the reason for  adding the auto encoder  which makes the model as noise free as  possible and focuses more on the key features  and the attention  mechanism which makes the model learning process focus on the key  areas  so the combination of the auto encoder and the dual attention  mechanism will make the model more robust. As shown in Fig. 2.  Then the content feature map is fed into the novel channels and position  attention networks  33 . In the position attention module  a  self attention mechanism is used to extract the feature map between any  two locations. For features at a particular location  any two locations with similar features can contribute to each other  s improvement by  aggregating the feature weights and updating them at all locations  and  for the channel attention module  a method similar to the self attention  mechanism is used to capture the channel dependent feature maps be  tween any two channels  and then the weighted sum is updated for each  channel map  then the two attention output modules are recently fused  that enhances the feature representation capability. We fuse the features  after the dual attention mechanism. The fused high dimensional features  represent the underlying semantic content of the image  after fusion the  content features should be reproduced into the original image and given  an image appearance code for each modality. To encourage the feature  extraction capability of the appearance code  we propose a loss function  to motivate modality specific decoders Dl M.  Lrepro  M i 1Dl M FusionCai  xi1  1   Our goal is to give fusion the robustness to produce high quality  reproductions  xi Dl M FusionCai even with missing modal data. And  Dl M FusionCai represents the novel encoding feature information ob  tained by fusing the features in the autoencoder of the appearance code  with the encoding of the image features obtained by fusion in the con  tent code. Then  in order to enhance the feature acquisition capability of  the autoencoder of the appearance code component  we calculate the  difference between the Dl M FusionCai and the feature value of the image  reconstructed by the image after the autoencoder and optimize the  difference to obtain the value of the Lrepro loss function. The objective  function for the entire model structure is   Loss total Loss cla   m i 1Loss repro  2     is the parameter weighting  m is the number of image modalities. In  this experiment we set   to 0.4  m is 2.  2.2. Network architecture  In the following section  we describe the proposed RMANet in detail.  RMANet mainly consists of a modality invariant feature part  modality  specific appearance feature part  as well as position attention  and  channel attention module  as shown in Table 1.   1  Modality invariant content feature part  In the previous network structure  a single backbone was used as the  feature extractor and only for a single modality  the cancerous region  may have different features in different modal MRI images  so a single  Fig. 1.The proposed robust multi modal  feature autoencoder attention net  RMA   Net  network structure. The blue part and  the green part represent the two branches.  The blue part represents the modality   invariant feature part. The green part rep  resents the modality specific appearance  part. The yellow dotted lines are position  and channel attention modules which is the  gray circle after the c1 cm feature value.   For interpretation of the references to  colour in this figure legend  the reader is  referred to the Web version of this article.     Fig. 2.The light green part is the position attention branch  the light blue part  is the channel attention branch. After c1 cm of feature vectors  the feature  values are resized to C  H  W and fed into the light green position attention  module and the light blue channel attention module respectively  finally the  feature map after the attention module is fed into the fully connected layer.  For  interpretation of the references to colour in this figure legend  the reader is  referred to the Web version of this article.  B. Li et al.                                                                                                                                                                                                                                        

Informatics in Medicine Unlocked 30  2022  100923 4model and a single modality cannot learn all the features of the  cancerous region perfectly. The features of the cancer region under other  modal images are lost  so we use a dual input model in order for the  network to learn the features of the cancer region in different modalities.  We input the MRI images of both modalities into a ten layer convolu   tional neural network for feature extraction. In this ten layer convolu   tional neural network  we resized all the original images size to 256   256 by selecting the center of each image as the center of the ROI and  ensured that the integrity of the prostate area was included. use a 3  3  convolutional kernels with a stride size of  1 1   activation is Relu  Max  pooling size is 2  2. After the feature extractor  the final layer of output  is a 32  32  512 vector  which is then fed into the channel attention  and attention modules respectively. After the attention module  the two  outputs are merged and flattened to obtain a one dimensional vector of  524288. The final output after the dense layer is two categories.   2  Modality specific appearance feature part  In the modality invariant feature part  we mainly learn the common  features of the two modality images  but in the different modality im  ages  there are different features in the cancerous region. So  in this part   we will mainly learn the detailed features of each modality  and we use  the classical U net  34  structure as the backbone that becomes an  auto encoder and uses unsupervised learning for efficient feature  extraction and feature representation of high dimensional data. We keep  the encoder part and the decoder part of the U net structure and learn  the detailed features of each modality through the self encoder  then  fuse the merged vector from the modality invariant feature part into the  vector encoded by the modality specific appearance feature part. The  output features of both the feature extraction and dual attention  mechanisms are 32  32  512. We fuse the two parts directly  and then  decode it to obtain the objectives loss function. By redesigning the loss of  the auto encoder and the loss of the classifier  we propose a new loss  function to update the whole network as shown in formula  2 .   3  Channel attention and position attention mechanisms  This self attention mechanism is used to enhance the discriminant  ability of feature representations for task classification. In our work  we mainly use the dual attention network proposed by Ref.  33   and add  the dual attention module to the modality invariant feature part. First   there is the position attention mechanism. After the convolution layer   there is a feature A ZC H W   which then goes into the attention  module. After that  we get the new feature map B and C  and then  reshape them into ZC N where N  H  W is the number of pixels. Then  we go through the SoftMax layer to calculate the position attention map  s ZN N  sji exp  Bi Cj   N i 1exp  Bi Cj   3   sji indicates the effect of position ith on position jth. Finally  we add  feature A to the computed feature map  the spatial attention module can  be denoted as   Ej   N i 1  sjiDi   Aj  4   Then next is the channel attention mechanism  we calculate the  channel attention map X ZC C directly from the original feature map  A ZC H W   so the channel attention mechanism is shown as   xji exp  Ai Aj   C i 1exp  Ai Aj   5   In the same way as spatial attention  xji represents the effect of  channel ith on channel jth  and the channel attention module can be  expressed as   Ej   C i 1  sjiAi   Aj  6    2.3. Dataset  1  Dataset selection  We collected T2 and DWI of the prostate to  evaluate the performance of the model proposed in this paper from two  different sites  including 179 samples from Toho University Medical  Centre  Dataset A   and 200 samples from 2017 SPIE AAPM NCI Pros  tateX challenge dataset  Dataset B   35 . The ProstateX Challenge    Table 1  Structural details.   Input Size 256  256 3 Input Size 32  32 512  CNN 1 M Encode 1 M Decoder 1 M PAM CAM  Operation  number size   stride  Output  h  w c  Operation Output Operation Output Operation Output Operation Output  64 3  3   1 1  256  256 64 3  3   1 1  256   256 512 3  3    1 1  32  32 512 3  3    1 1  32  32 512 3  3    1 1  32  32  64 3  3   1 1  256  256 64 3  3   1 1  256   256 512 3  3    1 1  32  32 BN 32  32 BN 32  32  Max pooling 2 128  128 Max pooling 2 128   128 Up sample 2 64  64 Activation 32  32 Activation 32  32  128 3  3   1 1  128  128 128 3  3    1 1  128   128 256 3  3    1 1  64  64 512 3  3    1 1  32  32 512 3  3    1 1  32  32  128 3  3   1 1  128  128 128 3  3    1 1  128   128 256 3  3    1 1  64  64      Max pooling 2 64  64 Max pooling 2 64  64 Up sample 2 128   128      256 3  3   1 1  64  64 256 3  3    1 1  64  64 128 3  3    1 1  128   128      256 3  3   1 1  64  64 256 3  3    1 1  64  64 128 3  3    1 1  128   128      Max pooling 2 64  64 Max pooling 2 64  64 Up sample 2 128   128      512 3  3   1 1  32  32 512 3  3    1 1  32  32 64 3  3   1 1  256   256      512 3  3   1 1  32  32 512 3  3    1 1  32  32 64 3  3   1 1  256   256      B. Li et al.                                                                                                                                                                                                                                        

Informatics in Medicine Unlocked 30  2022  100923 5SPIE AAPM NCI Prostate MR Classification Challenge    focused on  quantitative image analysis methods for the diagnostic classification of  clinically significant prostate cancers and was held in conjunction with  the 2017 SPIE Medical Imaging Symposium  among these two datasets   data is sourced from different sites and collected using different  appliances.  2 Data pre processing  We performed data pre processing on both  datasets  the raw images scanned the entire body  so we first crop the  data center of the 2 datasets to the prostate region  and then resize all the  data to 256  256 in the axial plane. To reduce the varying intensity  between samples of different data sets  we use the Transformer API in  scikit learn  36  to calculate the mean and standard deviation of the  data  then normalized each sample to have zero mean and unit variance  in intensity value before inputting to the network. We split the data into  five folds and cross validate them. Before the image is sent to the  network  the Keras comes with a data augmentation operation to  augment the image  using a rotation angle of 30  random shear of 0.2  scale images  random zoom in or out by 0.2x  and random horizontal  flip.  2.4. Implementations  All experiments in this work were performed with Python 3.6 on a  WINDOWS machine with an Nvidia TITAN RTX graphics card and 24 GB  of RAM  CPU model is Intel R  Core  TM  i7 9700K 3.60 GHz. Tensor   flow was used as the backend and Keras was used to build the archi   tecture  with both the dense and convolutional layers initialized with    he uniform    which is a He normal distribution initializer  that draws  samples from a truncated normal distribution centered at 0 with stan  dard deviation stddev  sqrt  2 fan in   where fan in is the number of  input units in the weight tensor. We used cross entropy loss as the  objective loss function  set the batch size to 2 and trained 200 epochs. In  the training process we used the Adam  37  optimization method   setting the initial learning rate to 1e 5 based on cross validated experi   ments and automatically multiplying the learning rate by 0.1 every 50  epochs. All data sets are randomly split into train  and validation sets on  a proportion of 60   30  and test sets are 10   respectively. Data were  divided into two categories according to the Gleason score  with Gleason  score less than or equal to 7 being clinically insignificant and Gleason  score greater than or equal to 8 being clinically significant. The network  is set to save the model with the best epoch results.  3.Method  3.1. Setup  We conduct pre processing for the two datasets. We first center   cropped the images from Dataset A and Dataset B from with roughly  same view. The method proposed in this paper is mainly used to predict  the high and low risk of early prostate cancer  according to the Gleason  score  a score greater than or equal to 8 is considered clinically severe  and less than or equal to 7 is considered clinically insignificant  43 44  .  We used three main evaluation metrics to evaluate the performance of  the model  AUC value  sensitivity  and specificity. AUC  Area Under  Curve  is defined as the area under the ROC curve enclosed by the co  ordinate axes. The ROC curve  known as the receiver operating char  acteristic curve  is a curve based on a series of different dichotomies   cut off values or decision thresholds   with the true positive rate   sensitivity  as the vertical coordinate and the false positive rate   1 specificity  as the horizontal coordinate.  sensitivity TP TP FN 7    specificity TN FP TN 8  In the above equation  TP  TN  FP  and FN  represent true negative   false positive and false negative respectively.  In the following sections  experiments are carried out to evaluate the  performance of the method proposed in this paper. The dual input im  ages are tested in Table 3  and the baseline method is compared in  Table 2. Comparison experiments with 3D CNN networks are shown in  Table 4 and comparison experiments with SOTA are shown in Table 5.  Comparative experiments with single and dual inputs are shown in  Table 6.  3.2. Comparison of baseline methods  We performed experiments used the same dataset on different  baseline methods to compare with the results obtained by our proposed  method  Table 2   in this section  we used VGG16  VGG19  Resnet50   Resnet101  and InceptionV3  38  as the baseline methods  and all these  methods use pretrained models trained on ImageNet. VGG was proposed  by the Visual Geometry Group at the University of Oxford to participate  in the ImageNet Image Classification and Localization Challenge in 2014  and achieved excellent results  ranking second in the classification task  and first in the localization task. The ResNet network is a reference to the  VGG19 network with modifications based on it and the inclusion of  residual units through a short circuiting mechanism. The changes are  mainly reflected in that ResNet directly uses the convolution with stride   2 for down sampling and replaces the fully connected layer with the  global average pool layer. An important design principle of ResNet is  that the number of feature maps is doubled when the feature map size is  reduced by half  which maintains the network layer complexity. In  practice  especially with medical images  the dataset is often not large  enough and few people train the network from scratch. It is common  practice to use a pre trained network  e.g.  a network trained on  ImageNet to classify 1000 classes  for re fine tuning  also called  fine tuning   or as a feature extractor  where we freeze the weights of the  first few layers of the model and retrain the latter layers. Compared with  the baseline method  it can be seen that our proposed model far sur  passes the baseline method in terms of sensitivity  specificity  and AUC  values  except for the model parameters that do not achieve the best  performance.  3.3. Comparison with dual input methodology  Table 2 shows the results obtained after a single modal input to the  baseline network  because of the limited features that can be learned  from a single modality. The method proposed in this paper also uses  images of two main modalities as input  we experiment with some of the  most basic two input models for comparison experiments. In the two   input model comparison experiments  we use images from the T2   DWI modality  as well as the model  s backbone from both baseline  models VGG16 and VGG19  and similarly  we use pre trained weights  after training on the ImageNet dataset. We input the images of both  modalities into the model at the same time  fuse the features after the  convolution layer  and then pass them through the fully connected layer  Table 2  Comparison with three classical backbone models of VGG  Resnet  Inception for  medical image classification and their fine tuned models for a total of seven  baseline methods using unimodal input methods.   Methods Sensitivity Specificity AUC CI 95  Parameters  VGG16 0.63 0.59 0.61 0.58  0.63 138 M  VGG16 fine turn 0.71 0.63 0.66 0.62  0.67 154 M  VGG19 0.61 0.54 0.58 0.52  0.63 143 M  VGG19 fine turn 0.65 0.54 0.59 0.56  0.62 156 M  Resnert50 0.69 0.61 0.65 0.63  0.68 46M  Resnet101 0.73 0.68 0.71 0.68  0.73 85 M  InceptionV3 0.67 0.59 0.63 0.58  0.66 247  Our method 0.84 0.78 0.84 0.78  0.84 94.1 M  B. Li et al.                                                                                                                                                                                                                                        

Informatics in Medicine Unlocked 30  2022  100923 6to obtain the output. Table 3 shows the results of the comparison ex  periments  it can be seen that our method performs much better than the  baseline two input model.  3.4. Comparison with 3D CNN methodology  Table 3 show the result of the performance on the 2D CNN network  and with 2D input images. However  in recent studies  increasing  research has focused on the classification of sequence images. The dif  ference between 2D CNN and 3D CNN is that 3D CNN is used to operate  on sequence images  generally using a CNN for each image of the  sequence separately  and the recognition in this way does not consider  the information between each successive image of the sequence in the z   dimension. The 3D CNN performs a convolution operation with a z   dimension of 3  i.e.  on three consecutive images in the sequence. The 3D  convolution above is performed by stacking multiple consecutive frames to form a cube  and then applies a 3D convolution kernel in the cube. In  this structure  each feature map in the convolution layer is connected to  multiple neighboring contiguous images in the previous layer  thus  capturing sequential information. The value of a particular position of a  convolution map is obtained by convolving the local perceptual field of  the same position of three consecutive images from the previous layer. In  our comparison experiments  we added the classical C3D  39  model of  recent years  which uses 3D convolution for feature learning on  sequence images. The size of the convolution kernel in the C3D model is  3  3  3  padding is 1  1  1  and compared to 2D images  sequence  images can be learned with more spatial features. In this experiment  we  mainly use DWI and T2 sequence images and try different input sizes  384  384  128  128. We try to reduce the input size by cropping the x  and y axes to keep only the prostate region as much as possible  while  keeping the z axis constant. Table 4 shows that our method still achieves  the best performance.  3.5. Comparison with state of the art methodology  We have also compared our proposed method with state of the art  methods. As shown in Table 5  including a method proposed by Aldoj  et al.  26  in 2020 for prostate cancer classification using multi channel  convolutional neural networks on multi modal MRI images. This  approach uses three types of images from two modalities T2  DWI  and  ADC  where the ADC image is obtained by calculating a specific b value  in DWI. A total of 11 layers of 3D convolution with a 3  3  3  convolution kernel  and a 2  2  2 pooling stride  and two fully con  nected layers. Because this method selects data from three modes in the  experiment  in order to balance the comparison of the experimental  results  we have selected only the results obtained from his paper for two  of the modes as input. We can see that the sensitivity  specificity  and  AUC values of our method are 0.1  0.08  and 0.055 higher  respectively   than the same 2 modal image input. In a 2019 study by Zhong et al.  27   on the classification of prostate cancer based on multimodal MRI images  using deep migration learning. It proposes to simultaneously feed im  ages of T2 and ADC modalities into a deep migration learning network  for feature extraction and obtain prediction results after a fully con  nected layer. In the comparison experiments of Zhong et al.  27   the  results were compared using unimodal and bimodal image inputs  and  here  for the sake of objectivity in comparison experiments  we only  select the results of their comparison experiments with dual modal in  puts  and we can see that the sensitivity  as well as AUC values have  increased by 0.204  0.112  respectively  in our model. In the approach  proposed by Chen et al.  28  in 2017 to classify the clinical severity of  prostate cancer using migration learning on the basis of multimodal  MRI  the authors mainly used migration learning and pre trained  weights obtained after training on ImageNet and used InseptionV3 for  their experiments. In comparison  the sensitivity of our method was  higher than the method of Chen et al.  28  by 0.06. We compared our  method with the other three methods in a one sample t test  and the  p value was 2.89e 6 when comparing with Zhong et al. method  5.57e 5  when comparing with Aldoj et al. method  and 0.16 when comparing  with Chen  s method  and based on the p value. There is a statistically  significant difference between our method and Zhong et al. and Aldoj  et al. methods  although the difference is not significant when  comparing with Chen et al. method  but the sensitivity is 0.06 higher  than Chen et al. method. And the sensitivity of our method and Chen  et al. method using a one sample t test was compared with a p value of  0.001  therefore  there was a significant difference in sensitivity  and the  sensitivity of our method was higher than that of Chen et al. method.  Because avoiding misdiagnosis is more important in clinical diagnosis   we chose sensitivity for comparison in the absence of significant dif  ferences in AUC values. We also conducted a comparison experiment  with Khosravi et al.  2021  46  method  and after comparing the clas  sification results of severe and non severe  the sensitivity  specificity and  AUC of our method were 0.13  0.09 and 0.06 higher than those of Table 3  Comparison with VGG classical backbone model with different number of layers  for medical image classification model with a total of 4 baseline methods using  dual modal input methods.   Methods Sensitivity Specificity AUC CI 95  Parameters  VGG16 VGG16 0.62 0.58 0.59 0.59  0.62 276 M  VGG16 VGG19 0.69 0.63 0.68 0.64  0.69 281 M  Dual input  VGG16 0.61 0.59 0.60 0.55  0.62 276 M  Dual input  VGG19 0.65 0.59 0.61 0.57  0.62 286 M  Our method 0.84 0.78 0.84 0.78  0.84 94.1 M   Table 4  Comparison with classical 3D CNN models with different modalities with  different input size for medical image classification  total 2 modalities and two  input sizes.   Methods Sensitivity Specificity AUC CI 95  Parameters  T2 384  0.63 0.59 0.68 0.66  0.69    T2 128  0.71 0.63 0.72 0.71  0.74    DWI 384  0.61 0.54 0.71 0.69  0.72    DWI 128  0.65 0.54 0.74 0.73  0.76    Our method 0.84 0.78 0.84 0.78  0.84 94.1 M   Table 5  Comparison with the 3 latest methods used to classify the clinical severity of  prostate cancer.   Methods Sensitivity Specificity AUC CI 95  Parameters  Zhong et al.   2019  27  0.636 0.80 0.723 0.58  0.88    Aldoj et al.  2020   26  0.74 0.70 0.78      Chen et al.  2017   28  0.78 0.83 0.83      Khosravi et al.   2021  46  0.71 0.69 0.78 0.74  0.82   Our method 0.84 0.78 0.84 0.78  0.84 94.1 M   Table 6  Comparison of the model proposed in our paper by using the two input modal  approach with the use of the single input modal T2  DWI.   Methods Sensitivity Specificity AUC CI 95  Parameters  Single input T2  image 0.71 0.69 0.69 0.67  0.72 47 M  Single input DWI  image 0.73 0.71 0.71 0.7 0.73 47 M  Dual input image 0.84 0.78 0.84 0.78  0.84 94.1 M  B. Li et al.                                                                                                                                                                                                                                        

Informatics in Medicine Unlocked 30  2022  100923 7Khosravi et al.  2021  46  method  respectively.  3.6. Comparison of different inputs of our method  In this section  we included ablation experiments with different  modal inputs. As can be seen from Table 6  we used single input T2  images  single input DWI images  and dual input which is our method in  this paper. From the results  the AUC value of the result of single input  T2 image is 0.02 less than the result of single input DWI image  and the  AUC value of the result of Dual input image is better than all baseline  methods.  3.7. Comparison without dual attention module  As shown in Fig. 3. We used the dual attention module as a detach   able module and used the Grad cam  40  to see if the attention mech   anism worked during the learning process. And as we can see in Fig. 4   the sensitivity  specificity and AUC values after using the attention  module are 0.015  0.03 and 0.025 higher than those before using the  attention module  respectively. The AUC values with and without the  dual attention model had 95  confidence intervals of 0.784  0.84 and  0.75  0.81  respectively. And we performed an independent t test on the  two groups of data with and without the dual attention module  we  obtained a p value of 0.0065  therefore there is a significant difference  between the data obtained by the two methods which means that the  dual attention module is effective.  For a long time  CNNs have been effective but controversial because  of their poor interpretability. For a deep convolutional neural network   the last convolutional layer contains the richest spatial and semantic  information through multiple convolutions and pooling  and the next  layer is the fully connected with SoftMax layers  which contain infor  mation that is difficult for humans to understand and difficult to  visualize.  4.Discussion  At present  the novel model proposed in this paper for clinical  severity classification of prostate cancer obtained well results  but we  can observe from Fig. 5 that the effect of the dual attention module  applied on the images of T2 modality is not as robust as on DWI mo  dality  in T2 images  although the dual attention module can make the  model focus on the prostate region  it does not focus on the cancerous  region as on DWI images  therefore  we remain to discuss on the  application of the dual attention module in different modalities and  improve the dual attention module under different modalities. And in  the experiments  the specific effects of the self encoder and the dual  attention module on the model require to be investigated in depth  we  would need to investigate the effect of different self encoder and dual  attention module on the results of using different modal images.  In Wang et al.  9 . study  they proposed a multi input convolutional  neural network to classify the clinical severity of prostate cancer and  obtained an AUC of 0.89 in a ProstateX dataset of 330 lesions. However   in this method  he divided the images of all modalities of each patient  and the images calculated from different b values into zones such as  peripheral zone central zone according to the prostate area and classi   fied them on different zones  this method of classifying the prostate area  severely consumes the physician  s time and requires a high profession   alism. In 2020 Aldoj et al.  26  work  although their proposed method  can achieve a maximum AUC value of 0.91  they used 4 modalities of  images for T2  ADC  DWI and K trans  and the images were manually  cropped to the prostate lesion area by the physician.  In future work  we should further explore the effects of different  pattern images on classification results and investigate the dual   attention model in depth.  Fig. 3.Examples of differences in attention maps with and without the dual  attention module  the red area is the area on which the model focuses .  For  interpretation of the references to colour in this figure legend  the reader is  referred to the Web version of this article.   Fig. 4.Differences in sensitivity  specificity and AUC values with and without  dual attention modules.  Fig. 5.Examples of differences in attention maps with and without the dual  attention module on T2 image  the red area is the area on which the model  focuses .  For interpretation of the references to colour in this figure legend  the  reader is referred to the Web version of this article.  B. Li et al.                                                                                                                                                                                                                                        

Informatics in Medicine Unlocked 30  2022  100923 85.Conclusion  In this paper  we propose a novel convolutional neural network  model RMAnet  which can predict prostate cancer for different modal   ities of MRI images and maintain robustness. This approach has been  particularly helpful in the diagnosis of the clinical severity of prostate  cancer. The model mainly includes a modality specific appearance part  and a modality invariant content part  as well as position and channel  attention modules. After experiments  the average AUC of our proposed  model reached 0.84  which is higher than most recent methods  the  sensitivity values  reaching 0.84  are higher than the recent method.  Acknowledgement  Throughout the writing of this study  I have received a great deal of  support and assistance.  I would first like to thank my supervisor  Prof. Toshiya Nakaguchi  whose expertise was invaluable in formulating the research questions  and methodology. Your insightful feedback pushed me to sharpen my  thinking and brought my work to a higher level.  I would particularly like to acknowledge my team members Dr. Ryo  Oka Prof Xuan Ping for their wonderful collaboration and patient  support  I would also like to thank my tutors Yuichiro  Yoshimura for their  valuable guidance throughout my studies. You provided me with the  tools that I needed to choose the right direction and successfully com  plete my dissertation.  In addition  I would like to thank my family for their wise counsel  and sympathetic ear. They are always there for me.  References   1 Mohler J  Bahnson RR  Boston B  Busby JE  D Amico A  Eastham JA  Enke CA   George D  Horwitz EM  Huben RP. others  Prostate cancer. J Natl Compr Cancer  Netw 2010 8 162  200.   2 Partin AW  Kattan MW  Subong ENP  Walsh PC  Wojno KJ  Oesterling JE   Scardino PT  Pearson JD. Combination of prostate specific antigen  clinical stage   and Gleason score to predict pathological stage of localized prostate cancer  a  multi institutional update. JAMA 1997 277 1445  51.   3 Wu G  Datar RH  Hansen KM  Thundat T  Cote RJ  Majumdar A. Bioassay of  prostate specific antigen  PSA  using microcantilevers. Nat Biotechnol 2001 19   856 60.   4 Catalona WJ  Richie JP  Ahmann FR  Hudson MA  Scardino PT  Flanigan RC   Dekernion JB  Ratliff TL  Kavoussi LR  Dalkin BL. others  Comparison of digital  rectal examination and serum prostate specific antigen in the early detection of  prostate cancer  results of a multicenter clinical trial of 6 630 men. J Urol 1994   151 1283  90.   5 Huettel SA  Song AW  McCarthy G. Functional magnetic resonance imaging. MA   Sinauer Associates Sunderland  2004. others .   6 Padhani AR  Liu G  Mu Koh D  Chenevert TL  Thoeny HC  Takahara T  Dzik   Jurasz A  Ross BD  van Cauteren M  Collins D. others  Diffusion weighted magnetic  resonance imaging as a cancer biomarker  consensus and recommendations.  Neoplasia 2009 11 102  25.   7 Duggirala B  Paine DS  Comaniciu D  Krishnan A  Zhou X. Computer aided  diagnostic assistance for medical imaging. 2007 .   8 Litjens G  Debats O  Barentsz J  Karssemeijer N  Huisman H. Computer aided  detection of prostate cancer in MRI. IEEE Trans Med Imag 2014 33 1083  92.   9 Wang Y  Wang M. Selecting proper combination of mpMRI sequences for prostate  cancer classification using multi input convolutional neuronal network. Phys Med  2020 80 92  100.   10  Abbasi AA  Hussain L  Awan IA  Abbasi I  Majid A  Nadeem MSA  Chaudhary Q A.  Detecting prostate cancer using deep learning convolution neural network with  transfer learning approach. Cognit Neurodynamics 2020 14 523  33.   11  LeCun Y  Bengio Y  Hinton G. Deep learning. Nature 2015 521 436  44.   12  Krizhevsky A  Sutskever I  Hinton GE. ImageNet classification with deep  convolutional neural networks. Commun ACM 2017 60 84  90.   13  Simonyan K  Zisserman A. Very deep convolutional networks for large scale image  recognition. 2014. ArXiv Preprint ArXiv 1409.1556 .   14  Szegedy C  Liu W  Jia Y  Sermanet P  Reed S  Anguelov D  Erhan D  Vanhoucke V   Rabinovich A. Going deeper with convolutions. In  Proceedings of the IEEE  conference on computer vision and pattern recognition  2015. p. 1 9.   15  He K  Zhang X  Ren S  Sun J. Deep residual learning for image recognition. In   Proceedings of the IEEE conference on computer vision and pattern recognition   2016. p. 770 8.  16  Litjens G  Kooi T  Bejnordi BE  Setio AAA  Ciompi F  Ghafoorian M  van der  Laak JA  van Ginneken B  S anchez CI. A survey on deep learning in medical image  analysis. Med Image Anal 2017 42 60  88.   17  Bar Y  Diamant I  Wolf L  Lieberman S  Konen E  Greenspan H. Chest pathology  detection using deep learning with non medical training. In  2015 IEEE 12th  international Symposium on biomedical imaging  ISBI   2015. p. 294 7.   18  Lundervold AS  Lundervold A. An overview of deep learning in medical imaging  focusing on MRI. Z Med Phys 2019 29 102  27.   19  Cheng J Z  Ni D  Chou Y H  Qin J  Tiu C M  Chang Y C  Huang C S  Shen D  Chen C   M. Computer aided diagnosis with deep learning architecture  applications to  breast lesions in US images and pulmonary nodules in CT scans. Sci Rep 2016 6   1 13.   20  Reda I  Khalil A  Elmogy M  Abou El Fetouh A  Shalaby A  Abou El Ghar M   Elmaghraby A  Ghazal M  El Baz A. Deep learning role in early diagnosis of  prostate cancer. Technol Cancer Res Treat 2018 17. 1533034618775530 .   21  Pan SJ  Yang Q. A survey on transfer learning. IEEE Trans Knowl Data Eng 2009   22 1345  59.   22  Shin H C  Roth HR  Gao M  Lu L  Xu Z  Nogues I  Yao J  Mollura D  Summers RM.  Deep convolutional neural networks for computer aided detection  CNN  architectures  dataset characteristics and transfer learning. IEEE Trans Med Imag  2016 35 1285  98.   23  Haider MA  van der Kwast TH  Tanguay J  Evans AJ  Hashmi A T  Lockwood G   Trachtenberg J. Combined T2 weighted and diffusion weighted MRI for  localization of prostate cancer. Am J Roentgenol 2007 189 323  8.   24  Khalifa F  Soliman A  El Baz A  Abou El Ghar M  El Diasty T  Gimel  farb G   Ouseph R  Dwyer AC. Models and methods for analyzing DCE MRI  a review. Med  Phys 2014 41 124301 .   25  Lim HK  Kim JK  Kim KA  Cho K S. Prostate cancer  apparent diffusion coefficient  map with T2 weighted images for detection  a multireader study. Radiology 2009   250 145  51.   26  Aldoj N  Lukas S  Dewey M  Penzkofer T. Semi automatic classification of prostate  cancer on multi parametric MR imaging using a multi channel 3D convolutional  neural network. Eur Radiol 2020 30 1243  53.   27  Zhong X  Cao R  Shakeri S  Scalzo F  Lee Y  Enzmann DR  Wu HH  Raman SS   Sung K. Deep transfer learning based prostate cancer classification using 3 Tesla  multi parametric MRI. Abdom Radiol 2019 44 2030  9.   28  Chen Q  Xu X  Hu S  Li X  Zou Q  Li Y. A transfer learning approach for  classification of clinical significant prostate cancers from mpMRI scans. In  Medical  imaging 2017  computer aided diagnosis  2017. 101344F .   29  Castillo T JM  Arif M  Niessen WJ  Schoots IG  Veenland JF. others  Automated  classification of significant prostate cancer on MRI  a systematic review on the  performance of machine learning applications. Cancers 2020 12 1606 .   30  Chartsias A  Joyce T  Giuffrida MV  Tsaftaris SA. Multimodal MR synthesis via  modality invariant latent representation. IEEE Trans Med Imag 2017 37 803  14.   31  Ben Cohen A  Mechrez R  Yedidia N  Greenspan H. Improving CNN training using  disentanglement for liver lesion classification in CT. In  2019 41st annual  international conference of the IEEE engineering in medicine and biology society   EMBC   2019. p. 886 9.   32  Chen C  Dou Q  Jin Y  Chen H  Qin J  Heng P A. Robust multimodal brain tumor  segmentation via feature disentanglement and gated fusion. In  International  conference on medical image computing and computer assisted intervention  2019.  p. 447 56.   33  Fu J  Liu J  Tian H  Li Y  Bao Y  Fang Z  Lu H. Dual attention network for scene  segmentation. In  Proceedings of the IEEE CVF conference on computer vision and  pattern recognition  2019. p. 3146  54.   34  Ronneberger O  Fischer P  Brox T. U net  convolutional networks for biomedical  image segmentation. In  International conference on medical image computing and  computer assisted intervention  2015. p. 234 41.   35  Armato SG  Huisman H  Drukker K  Hadjiiski L  Kirby JS  Petrick N  Redmond G   Giger ML  Cha K  Mamonov A. others  PROSTATEx Challenges for computerized  classification of prostate lesions from multiparametric magnetic resonance images.  J Med Imag 2018 5 44501 .   36  Pedregosa F  Varoquaux G  Gramfort A  Michel V  Thirion B  Grisel O  Blondel M   Prettenhofer P  Weiss R  Dubourg V. others  Scikit learn  machine learning in  Python. J Mach Learn Res 2011 12 2825  30.   37  Kingma DP  Ba J. Adam  a method for stochastic optimization. 2014. ArXiv  Preprint ArXiv 1412.6980 .   38  Xia X  Xu C  Nan B. Inception v3 for flower classification. In  2017 2nd  international conference on image  vision and computing. ICIVC   2017. p. 783 7.   39  Tran D  Bourdev L  Fergus R  Torresani L  Paluri M. Learning spatiotemporal  features with 3d convolutional networks. In  Proceedings of the IEEE international  conference on computer vision  2015. p. 4489  97.   40  Selvaraju RR  Cogswell M  Das A  Vedantam R  Parikh D  Batra D. Grad cam  Visual  explanations from deep networks via gradient based localization. In  Proceedings  of the IEEE international conference on computer vision  2017. p. 618 26.   41  Ayyad SM  Shehata M  Shalaby A  El Ghar A  Ghazal M  El Melegy M  Abdel   Hamid NB  Labib LM  Ali HA  El Baz A. Role of AI and histopathological images in  detecting prostate cancer  a survey. Sensors 2021 21 8  2586. Jan.   42  Abdelmaksoud IR  Shalaby A  Mahmoud A  Elmogy M  Aboelfetouh A  El Ghar A   El Melegy M  Alghamdi NS  El Baz A. Precise identification of prostate cancer from  DWI using transfer learning. Sensors 2021 21 11  3664. Jan.   43  Epstein JI  Zelefsky MJ  Sjoberg DD  Nelson JB  Egevad L  Magi Galluzzi C   Vickers AJ  Parwani AV  Reuter VE  Fine SW  Eastham JA. A contemporary prostate  cancer grading system  a validated alternative to the Gleason score. Eur Urol 2016   69 3  428  35. Mar 1. B. Li et al.                                                                                                                                                                                                                                        

Informatics in Medicine Unlocked 30  2022  100923 9 44  Stark JR  Perner S  Stampfer MJ  Sinnott JA  Finn S  Eisenstein AS  Ma J   Fiorentino M  Kurth T  Loda M  Giovannucci EL. Gleason score and lethal prostate  cancer  does 3 4 4 3  J Clin Oncol 2009 27 21  3459. Jul 20.   45  Lalkhen  Abdul Ghaaliq  McCluskey Anthony. Clinical tests  sensitivity and  specificity. Cont Educ Anaesth Crit Care Pain 2008 8 6  221  3.  46  Khosravi P  Lysandrou M  Eljalby M  et al. A deep learning approach to diagnostic  classification of prostate cancer using pathology  radiology fusion J . J Magn  Reson Imag 2021 54 2  462  71. B. Li et al.                                                                                                                                                                                                                                        

