{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9e923374-d27c-427d-8d0a-0ca376c02b47",
   "metadata": {},
   "source": [
    "!pip install langchain-nomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3699f76f-4107-47c6-9bb6-b11785278605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af63e6-da01-488d-93be-ca016d72d2b2",
   "metadata": {},
   "source": [
    "# Load document and chunk it into smaller \"sub documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cde66a5-89fc-45d7-8b7b-11846ba68396",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"article/ziegler.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf74f75-4606-45c0-bd2b-68755d0c655e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f9dbbc-f78d-4961-b5e4-615e6d7ff064",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "pages = text_splitter.split_documents(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9523b138-b081-4489-8057-75d84acc17d2",
   "metadata": {},
   "source": [
    "# Create LLM | embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb205e8a-a4fa-48f6-bc84-4f4e08eb89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.environ.get('GROQ_API_KEY')\n",
    "HUGGINGFACE_API_TOKEN = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "os.environ[\"NOMIC_API_KEY\"] = os.environ.get('NOMIC_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3220d2d7-d513-4bed-9a86-e4141293700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32961229-150e-475c-8556-cbe061767cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "152f60d2-89c2-4807-adc5-ab618f89e9a9",
   "metadata": {},
   "source": [
    "llm = OllamaLLM(model=\"llama3.1:latest\",\n",
    "               base_url=\"http://192.168.1.115:11434\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcada1f0-9d3a-4b70-b96a-67348a397de0",
   "metadata": {},
   "source": [
    "respond = llm.invoke(\"Come up with 10 names for a song about parrots\")\n",
    "print(respond.content)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc3ee14b-4183-4aaa-bace-e65d6f250afa",
   "metadata": {},
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\",\n",
    "                             base_url=\"http://192.168.1.115:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc31ec-042f-469d-aab1-59a629e59412",
   "metadata": {},
   "source": [
    "# Create retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cea0fd-1687-49ac-90f2-eca61d2100e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./db\")\n",
    "give souce\n",
    "vector_store.add_documents(documents=pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6efe9b5-c4a6-46d8-ba29-bbb8130ba4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=\"./db\",\n",
    "            embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc2ac34f-30aa-47c2-bfe5-d79d36c47fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcef55db-fac8-41e8-b40b-a19ba4e3bda3",
   "metadata": {},
   "source": [
    "%%time\n",
    "vector_store = InMemoryVectorStore(embedding=embeddings)\n",
    "vector_store.add_documents(documents=pages)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ad1ab4d-7ee1-49ad-85da-b31736982dd0",
   "metadata": {},
   "source": [
    "%%time\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55fe064d-f72f-47ee-ac6c-30b73f19812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Chroma', 'NomicEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x7f702a7e66d0> search_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "print(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b264e5-9a32-4575-a138-2fbaaae8357e",
   "metadata": {},
   "source": [
    "# Create prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b37aaf20-58f1-4a02-8fd2-a1906e5d7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<bos><start_of_turn>user\\nAnswer the question based only on the following context and extract out a meaningful answer. \\\n",
    "Please write in full sentences with correct spelling and punctuation. if it makes sense use lists. \\\n",
    "If the context doesn't contain the answer, just respond that you are unable to find an answer. \\\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\\n\n",
    "ANSWER:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33533a69-5ffc-465a-8988-b140a71309ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef205a-4163-44ff-8e4a-5e75b1db779f",
   "metadata": {},
   "source": [
    "# Create the RAG chain using LCEL with prompt printing and streaming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d64c9a21-8acf-45e8-acf4-1638be7be03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0ccdf-c427-402c-8132-5692967701b8",
   "metadata": {},
   "source": [
    "# Function to ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51a2be4f-8512-4cb5-bcf4-fd3edd7368f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    print(\"Answer:\\n\\n\", end=\" \", flush=True)\n",
    "    for chunk in rag_chain.stream(question):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d263543-f0f6-46a2-82f0-d93f037cf954",
   "metadata": {},
   "source": [
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b87c7a-25df-451b-b682-0534714a70a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  list the tables and its description mention in the text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " Based on the provided context, I was able to extract the following tables and their descriptions:\n",
      "\n",
      "1. **Table 9**: Agreement probabilities for two tasks, i.e. the probability of both individuals choosing the same sample as best, out of 4. of labelers, and pairs of authors.\n",
      "2. **Table 10**: Mock sentiment continuation without a KL penalty.\n",
      "3. **Table 11**: Sentiment continuation.\n",
      "4. **Table 12**: Continuations for the descriptiveness continuation task, with contexts randomly sampled from BookCorpus test set.\n",
      "5. **Table 13 to 15**: TL;DR summarization.\n",
      "6. **Table 16 to 18**: CNN/Daily Mail summarization.\n",
      "\n",
      "Please let me know if you would like me to extract any other information.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  give the extract from table 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " I am unable to find an answer to the question because there is no Table 16 mentioned in the provided context.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  give the extract from cnn/daily mail summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " Based on the provided context, I was unable to find an answer that directly matches the question \"give the extract from CNN/Daily Mail summarization\". However, I can extract relevant information related to the CNN/Daily Mail summarization task.\n",
      "\n",
      "From Table 7, we can see that the frequency with which generated summaries copy the first 3 words of the article as the first 3 words of the summary is:\n",
      "\n",
      "* Zero-shot: 6/30\n",
      "* 60k fine-tuned: 26/30\n",
      "* Supervised: 8/30\n",
      "* Supervised + 60k fine-tuned: 11/30\n",
      "\n",
      "From Table 4, we can see the ROUGE evaluations of summarization models on the CNN/Daily Mail test set:\n",
      "\n",
      "* 60k fine-tuned: 16.800 R-1, 2.884 R-2, 14.011 R-L, 11.232 R-A\n",
      "* Supervised: 17.535 R-1, 3.124 R-2, 14.969 R-L, 11.877 R-A\n",
      "* Supervised + 60k fine-tuned: 18.434 R-1, 3.542 R-2, 15.457 R-L, 12.478 R-A\n",
      "* 60k ofï¬‚ine fine-tuned: 16.632 R-1, 2.699 R-2, 13.984 R-L, 11.105 R-A\n",
      "\n",
      "From Table 5, we can see the human evaluation of summarization models on the CNN/Daily Mail test set:\n",
      "\n",
      "* 60k fine-tuned vs. zero-shot: 96% 4% 91% 9%\n",
      "* 60k fine-tuned vs. supervised: 97% 3% 80% 20%\n",
      "* 60k fine-tuned vs. lead-3: 45% 55% 40% 60%\n",
      "* 60k fine-tuned vs. supervised + 60k fine-tuned: 80% 20% 74% 26%\n",
      "\n",
      "Please note that these are just a few extracts related to the CNN/Daily Mail summarization task, and not the exact extract requested. If you would like me to clarify or provide more information, please let me know!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  what is ROUGE evaluations?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " Based on the provided context, ROUGE evaluations refer to a set of numerical results that measure the performance of summarization models. Specifically, ROUGE (Recall-Oriented Understudy for GIST Evaluation) is a popular evaluation metric for summarization tasks, which assesses the similarity between a generated summary and a reference summary. The ROUGE results in the context are presented in Table 4, which compares the performance of different summarization models on the CNN/Daily Mail and TL;DR datasets.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  what measurement are used in the text?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " Based on the provided context, the following measurements are mentioned:\n",
      "\n",
      "1. **n-grams**: consecutive sequences of words in a single sentence in a summary, which count as novel if they do not appear consecutively in the article.\n",
      "2. **Bigrams**: consecutive sequences of two words in a summary, used to illustrate the location of the longest common subsequence between the context article/post and the summary.\n",
      "3. **Agreement probabilities**: the probability of both individuals choosing the same sample as best, out of 4 (Table 9).\n",
      "4. **Sentiment**: the probability of both individuals choosing the same sentiment (Table 9).\n",
      "5. **Descriptiveness**: the probability of both individuals choosing the same level of vividness in their descriptions (Table 9).\n",
      "6. **Confidence model**: a model used to evaluate the performance of each labeler on the benchmark distribution.\n",
      "7. **Rate of agreement**: the rate of agreement between authors and Scale labelers, pairs of authors, and between random responses (Table 9).\n",
      "8. **Entropy**: a measure used to target natural language in the fine-tuning process (Table 10).\n",
      "\n",
      "These measurements are used to evaluate the performance of the language models and the quality of the human-labeled data.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'quit' to exit):  give overview on the experiment done?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " Based on the provided context, I will extract a meaningful answer to the question. \n",
      "\n",
      "The experiment done is a fine-tuning of language models from human preferences. The objective is to investigate how human preferences can be used to improve the performance of language models. The experiment is designed to test the effectiveness of different models in generating summaries that are more accurate and informative than the original text.\n",
      "\n",
      "The experiment consists of several tasks, including sentiment analysis, descriptiveness evaluation, and summarization. The models are trained on a dataset of articles from CNN and Daily Mail, and are evaluated on their ability to generate summaries that accurately capture the main points of the text.\n",
      "\n",
      "The experiment uses a combination of human labeling and machine learning algorithms to evaluate the performance of the models. Human labelers are used to evaluate the quality of the summaries generated by the models, and the results are used to fine-tune the models to improve their performance.\n",
      "\n",
      "The experiment also includes an analysis of the agreement between human labelers and the models, and the results show that the models are able to generate summaries that are consistent with the human labels. This suggests that the models are able to capture the main points of the text and generate summaries that are accurate and informative.\n",
      "\n",
      "Overall, the experiment provides insights into the effectiveness of using human preferences to fine-tune language models, and suggests that this approach can be a useful tool for improving the performance of language models in various tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_question = input(\"Ask a question (or type 'quit' to exit): \")\n",
    "        if user_question.lower() == 'quit':\n",
    "            break\n",
    "        answer = ask_question(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544e125-f988-4dbb-a2f4-2ea415c6d2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
