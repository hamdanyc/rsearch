<?xml version="1.0" encoding="UTF-8"?>
<map version="1.0.1">
  <node TEXT="topic">
    <node TEXT="machine learning">
      <node TEXT="This is an open access article under the CC BY NC ND license   http   creativecommons.org licenses by  nc nd 4.0   .Robust multi modal prostate cancer classification via feature autoencoder  and dual attention  Bochong Lia    Ryo Oka  M.Db  Ping Xuan  Professorc  Yuichiro Yoshimura  PhDd   Toshiya Nakaguchi  Professore  aGraduate School of Science and Technology  Chiba University  1 33  Yayoicho  Inage Ward  Chiba shi  263 002  Japan  bToho University Sakura Medical Center  564 1 Shimoshizu  Sakura  Chiba  Sakura  285 0841  Japan  cSchool of Computer Science and Technology  Heilongjiang University  74 Xuefu Road  Nangang District  Harbin  Harbin  150080  China  dToyama University School of Medicine  3190 Gofuku  Toyama  Gofuku  930 8555  Japan  eCenter for Frontier Medical Engineering  Chiba University  1 33  Yayoicho  Inage Ward  Chiba shi  Chiba  263 8522  Japan    ARTICLE INFO   Keywords   Prostate cancer  Computer aided detection  Magnetic resonance imaging  Machine learning ABSTRACT   Prostate cancer is the second leading cause of cancer death in men. In this paper  we propose a novel and effective multi modal convolutional neural  network for discriminating prostate cancer clinical severity grade  i.e. Experiments were con  ducted on the ProstateX dataset and augmented with hospital data. In conventional diagnosis  there are two  main ways to confirm the diagnosis of prostate cancer  and one is  Prostate Specific Antigen  PSA   3   PSA is tissue specific and is only  found in the cytoplasm of human prostate alveoli and ductal epithelial  cells and is not expressed in other cells. Therefore  we require a non invasive method  with superior sensitivity and specificity to assist physicians in diag  nosing the clinical severity of prostate cancer. In recent years  prostate cancers are increasingly being diagnosed by  Magnetic Resonance Imaging  MRI  images. Contents lists available at ScienceDirect  Informatics in Medicine Unlocked  u   zkw  s yo kr o     1ow  o to 1m y2w m k o2ty   https   doi.org 10.1016 j.imu.2022.100923  Received 31 January 2022  Received in revised form 13 March 2022  Accepted 18 March 2022" />
      <node TEXT="I would first like to thank my supervisor  Prof. Toshiya Nakaguchi  whose expertise was invaluable in formulating the research questions  and methodology. 4 Catalona WJ  Richie JP  Ahmann FR  Hudson MA  Scardino PT  Flanigan RC   Dekernion JB  Ratliff TL  Kavoussi LR  Dalkin BL. 17  Bar Y  Diamant I  Wolf L  Lieberman S  Konen E  Greenspan H. Chest pathology  detection using deep learning with non medical training. 27  Zhong X  Cao R  Shakeri S  Scalzo F  Lee Y  Enzmann DR  Wu HH  Raman SS   Sung K. Deep transfer learning based prostate cancer classification using 3 Tesla  multi parametric MRI. 36  Pedregosa F  Varoquaux G  Gramfort A  Michel V  Thirion B  Grisel O  Blondel M   Prettenhofer P  Weiss R  Dubourg V. others  Scikit learn  machine learning in  Python. 40  Selvaraju RR  Cogswell M  Das A  Vedantam R  Parikh D  Batra D. Grad cam  Visual  explanations from deep networks via gradient based localization. Jan.   43  Epstein JI  Zelefsky MJ  Sjoberg DD  Nelson JB  Egevad L  Magi Galluzzi C   Vickers AJ  Parwani AV  Reuter VE  Fine SW  Eastham JA." />
      <node TEXT="Computer aided diagnostic  CAD   7  techniques based on machine  learning methods have been used extensively for the prediction and  detection of early prostate cancer  8 10 . Several machine learning methods are now avail  able to classify the clinical severity of prostate cancer. Recently  there has been an in crease in the use of deep learning based  CAD techniques for medical imaging data such as MRI  18  and CT   19   and these techniques have been used to solve the problem of early  prostate cancer prediction  20 . In some specific objectives  it is often  difficult for us to get enough data sets for feature extraction  so we have  to use the power of pre trained models including VGG16  13   VGG19   Resnet50  and Resnet101  15 . According to Lalken  s study  45   for early cancer screening   high sensitivity is more important to physicians and patients than  specificity  and our method achieves a higher sensitivity than other  methods. 2  We use autoencoders for the first time in a multimodal classification  network and combine a novel robust multimodal attention network  to im prove the accuracy of early prostate cancer prediction. RMANet  In this paper  we propose a novel convolutional neural network   CNN  architecture  i.e." />
    </node>
    <node TEXT="model">
      <node TEXT="However  in recent studies  increasing  research has focused on the classification of sequence images. In  our comparison experiments  we added the classical C3D  39  model of  recent years  which uses 3D convolution for feature learning on  sequence images. Table 4 shows that our method still achieves  the best performance. As shown in Table 5  including a method proposed by Aldoj  et al.  26  in 2020 for prostate cancer classification using multi channel  convolutional neural networks on multi modal MRI images. This  approach uses three types of images from two modalities T2  DWI  and  ADC  where the ADC image is obtained by calculating a specific b value  in DWI. It proposes to simultaneously feed im  ages of T2 and ADC modalities into a deep migration learning network  for feature extraction and obtain prediction results after a fully con  nected layer. Methods Sensitivity Specificity AUC CI 95  Parameters  T2 384  0.63 0.59 0.68 0.66  0.69    T2 128  0.71 0.63 0.72 0.71  0.74    DWI 384  0.61 0.54 0.71 0.69  0.72    DWI 128  0.65 0.54 0.74 0.73  0.76    Our method 0.84 0.78 0.84 0.78  0.84 94.1 M   Table 5  Comparison with the 3 latest methods used to classify the clinical severity of  prostate cancer." />
      <node TEXT="Informatics in Medicine Unlocked 30  2022  100923 5SPIE AAPM NCI Prostate MR Classification Challenge    focused on  quantitative image analysis methods for the diagnostic classification of  clinically significant prostate cancers and was held in conjunction with  the 2017 SPIE Medical Imaging Symposium  among these two datasets   data is sourced from different sites and collected using different  appliances. We split the data into  five folds and cross validate them. Implementations  All experiments in this work were performed with Python 3.6 on a  WINDOWS machine with an Nvidia TITAN RTX graphics card and 24 GB  of RAM  CPU model is Intel R  Core  TM  i7 9700K 3.60 GHz. Tensor   flow was used as the backend and Keras was used to build the archi   tecture  with both the dense and convolutional layers initialized with    he uniform    which is a He normal distribution initializer  that draws  samples from a truncated normal distribution centered at 0 with stan  dard deviation stddev  sqrt  2 fan in   where fan in is the number of  input units in the weight tensor. Data were  divided into two categories according to the Gleason score  with Gleason  score less than or equal to 7 being clinically insignificant and Gleason  score greater than or equal to 8 being clinically significant. Comparative experiments with single and dual inputs are shown in  Table 6. The method proposed in this paper also uses  images of two main modalities as input  we experiment with some of the  most basic two input models for comparison experiments." />
      <node TEXT="Each modal xi is fed into the respective  appearance encoder Ea i and respective CNNu i   after CNNu i   the model can  get the content feature map. In the position attention module  a  self attention mechanism is used to extract the feature map between any  two locations. For features at a particular location  any two locations with similar features can contribute to each other  s improvement by  aggregating the feature weights and updating them at all locations  and  for the channel attention module  a method similar to the self attention  mechanism is used to capture the channel dependent feature maps be  tween any two channels  and then the weighted sum is updated for each  channel map  then the two attention output modules are recently fused  that enhances the feature representation capability. To encourage the feature  extraction capability of the appearance code  we propose a loss function  to motivate modality specific decoders Dl M.  Lrepro  M i 1Dl M FusionCai  xi1  1   Our goal is to give fusion the robustness to produce high quality  reproductions  xi Dl M FusionCai even with missing modal data. In  this experiment we set   to 0.4  m is 2. RMANet mainly consists of a modality invariant feature part  modality  specific appearance feature part  as well as position attention  and  channel attention module  as shown in Table 1. 1  Modality invariant content feature part  In the previous network structure  a single backbone was used as the  feature extractor and only for a single modality  the cancerous region  may have different features in different modal MRI images  so a single  Fig." />
      <node TEXT="Comparison without dual attention module  As shown in Fig. For a long time  CNNs have been effective but controversial because  of their poor interpretability. For a deep convolutional neural network   the last convolutional layer contains the richest spatial and semantic  information through multiple convolutions and pooling  and the next  layer is the fully connected with SoftMax layers  which contain infor  mation that is difficult for humans to understand and difficult to  visualize. 4.Discussion  At present  the novel model proposed in this paper for clinical  severity classification of prostate cancer obtained well results  but we  can observe from Fig. However   in this method  he divided the images of all modalities of each patient  and the images calculated from different b values into zones such as  peripheral zone central zone according to the prostate area and classi   fied them on different zones  this method of classifying the prostate area  severely consumes the physician  s time and requires a high profession   alism. In 2020 Aldoj et al.  26  work  although their proposed method  can achieve a maximum AUC value of 0.91  they used 4 modalities of  images for T2  ADC  DWI and K trans  and the images were manually  cropped to the prostate lesion area by the physician. In future work  we should further explore the effects of different  pattern images on classification results and investigate the dual   attention model in depth." />
      <node TEXT="Computer aided diagnostic  CAD   7  techniques based on machine  learning methods have been used extensively for the prediction and  detection of early prostate cancer  8 10 . Several machine learning methods are now avail  able to classify the clinical severity of prostate cancer. Recently  there has been an in crease in the use of deep learning based  CAD techniques for medical imaging data such as MRI  18  and CT   19   and these techniques have been used to solve the problem of early  prostate cancer prediction  20 . In some specific objectives  it is often  difficult for us to get enough data sets for feature extraction  so we have  to use the power of pre trained models including VGG16  13   VGG19   Resnet50  and Resnet101  15 . According to Lalken  s study  45   for early cancer screening   high sensitivity is more important to physicians and patients than  specificity  and our method achieves a higher sensitivity than other  methods. 2  We use autoencoders for the first time in a multimodal classification  network and combine a novel robust multimodal attention network  to im prove the accuracy of early prostate cancer prediction. RMANet  In this paper  we propose a novel convolutional neural network   CNN  architecture  i.e." />
    </node>
    <node TEXT="supervised">
      <node TEXT="We input the MRI images of both modalities into a ten layer convolu   tional neural network for feature extraction. So  in this part   we will mainly learn the detailed features of each modality  and we use  the classical U net  34  structure as the backbone that becomes an  auto encoder and uses unsupervised learning for efficient feature  extraction and feature representation of high dimensional data. In our work  we mainly use the dual attention network proposed by Ref. After the convolution layer   there is a feature A ZC H W   which then goes into the attention  module. After that  we get the new feature map B and C  and then  reshape them into ZC N where N  H  W is the number of pixels. Then  we go through the SoftMax layer to calculate the position attention map  s ZN N  sji exp  Bi Cj   N i 1exp  Bi Cj   3   sji indicates the effect of position ith on position jth. Finally  we add  feature A to the computed feature map  the spatial attention module can  be denoted as   Ej   N i 1  sjiDi   Aj  4   Then next is the channel attention mechanism  we calculate the  channel attention map X ZC C directly from the original feature map  A ZC H W   so the channel attention mechanism is shown as   xji exp  Ai Aj   C i 1exp  Ai Aj   5   In the same way as spatial attention  xji represents the effect of  channel ith on channel jth  and the channel attention module can be  expressed as   Ej   C i 1  sjiAi   Aj  6    2.3." />
    </node>
  </node>
</map>
